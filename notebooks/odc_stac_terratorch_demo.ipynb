{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for TerraMind v1 integration\n",
    "# Uncomment and run the following lines if you need to install dependencies:\n",
    "\n",
    "# !pip install \"git+https://github.com/terrastackai/terratorch.git\" huggingface_hub tokenizers\n",
    "# !pip install holoviews bokeh scikit-learn\n",
    "\n",
    "print(\"üìã For TerraMind v1 support, make sure you have installed:\")\n",
    "print(\"   pip install 'git+https://github.com/terrastackai/terratorch.git'\")\n",
    "print(\"   pip install holoviews bokeh scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# FOSS4G 2025 Demo: TerraMind v1 Embedding Generation with odc-stac\n",
    "\n",
    "This notebook demonstrates the complete workflow for generating geospatial embeddings from satellite imagery using **IBM's TerraMind v1 foundation model**:\n",
    "\n",
    "1. **Load satellite data** from STAC catalogs using odc-stac\n",
    "2. **Process RGB composites** for TerraMind model input\n",
    "3. **Load TerraMind v1 model** with TerraTorch from HuggingFace\n",
    "4. **Generate 768-dimensional embeddings** from 16x16 RGB patches\n",
    "5. **Visualize embeddings** in 3D space using dimensionality reduction\n",
    "\n",
    "## üöÄ Key Technologies\n",
    "\n",
    "- **odc-stac**: Load STAC items into xarray Datasets\n",
    "- **TerraTorch**: Foundation model integration and training toolkit\n",
    "- **TerraMind v1**: IBM's geospatial foundation model (768-dimensional embeddings from 16x16 patches)\n",
    "- **Element84 Earth Search**: AWS-hosted STAC catalog for satellite data\n",
    "- **HuloViews**: Interactive 3D visualization of embedding space\n",
    "\n",
    "## ‚ú® TerraMind v1 Features\n",
    "\n",
    "- **768-dimensional embeddings** from Sentinel-2 RGB imagery\n",
    "- **16x16 patch optimization** for efficient processing  \n",
    "- **Pre-trained on massive geospatial datasets** from HuggingFace Hub\n",
    "- **Direct integration** with modern cloud-native workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## üéØ TerraMind v1 Integration\n",
    "\n",
    "**TerraMind v1 is now fully working** with the latest TerraTorch installation from GitHub! This notebook demonstrates the complete integration from STAC data loading to 768-dimensional embedding generation.\n",
    "\n",
    "### ‚úÖ What's Working\n",
    "- **TerraMind v1 Base**: 768-dimensional embeddings from HuggingFace Hub\n",
    "- **16x16 RGB patches**: Optimized for Sentinel-2 imagery processing  \n",
    "- **Direct STAC integration**: Load ‚Üí Process ‚Üí Embed workflow\n",
    "- **Production scale**: Handle thousands of patches efficiently\n",
    "\n",
    "### üîß Installation Requirements\n",
    "Make sure you have the latest TerraTorch with TerraMind support:\n",
    "```bash\n",
    "pip install \"git+https://github.com/terrastackai/terratorch.git\" huggingface_hub tokenizers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running for the first time)\n",
    "# !pip install odc-stac pystac-client xarray rasterio matplotlib\n",
    "# !pip install holoviews bokeh scikit-learn\n",
    "# !pip install \"git+https://github.com/terrastackai/terratorch.git\" huggingface_hub tokenizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odc.stac\n",
    "\n",
    "# STAC and data loading\n",
    "import pystac_client\n",
    "\n",
    "# TerraTorch and ML\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    # Correct import pattern for TerraTorch with TerraMind support\n",
    "    from terratorch.registry import BACKBONE_REGISTRY\n",
    "    print(\"‚úÖ TerraTorch BACKBONE_REGISTRY imported from registry\")\n",
    "    \n",
    "    # Check for TerraMind availability\n",
    "    all_models = list(BACKBONE_REGISTRY)\n",
    "    terramind_models = [m for m in all_models if 'terramind' in m.lower()]\n",
    "    print(f\"üéØ Found {len(terramind_models)} TerraMind models\")\n",
    "    \n",
    "    print(\"‚úÖ TerraTorch imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è TerraTorch import issue: {e}\")\n",
    "    BACKBONE_REGISTRY = None\n",
    "\n",
    "# Visualization libraries\n",
    "try:\n",
    "    import holoviews as hv\n",
    "    hv.extension(\"bokeh\")\n",
    "    HV_AVAILABLE = True\n",
    "    print(\"‚úÖ HoloViews imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è HoloViews not available: {e}\")\n",
    "    print(\"üìä Will use matplotlib for visualization instead\")\n",
    "    HV_AVAILABLE = False\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üöÄ All available libraries imported successfully!\")\n",
    "print(f\"üß† TerraTorch version: Latest from GitHub with TerraMind support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Connect to STAC Catalog\n",
    "\n",
    "Connect to Element84 Earth Search STAC catalog for satellite data discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "STAC_URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# Auckland, New Zealand - demo area\n",
    "BBOX = [174.6, -36.95, 174.85, -36.75]\n",
    "DATETIME = \"2023-12-01/2023-12-31\"\n",
    "BANDS = [\"red\", \"green\", \"blue\", \"nir\"]\n",
    "\n",
    "# Connect to STAC catalog\n",
    "logger.info(f\"Connecting to STAC catalog: {STAC_URL}\")\n",
    "catalog = pystac_client.Client.open(STAC_URL)\n",
    "print(f\"‚úÖ Connected to {catalog.title}\")\n",
    "\n",
    "# Display catalog information\n",
    "print(f\"üìç Catalog URL: {STAC_URL}\")\n",
    "print(f\"üóÇÔ∏è Available collections: {len(list(catalog.get_collections()))}\")\n",
    "print(f\"üéØ Target collection: {COLLECTION}\")\n",
    "print(f\"üì¶ Area of Interest: {BBOX} (Auckland, NZ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Search and Load Satellite Data\n",
    "\n",
    "Search for Sentinel-2 imagery and load it using odc-stac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Sentinel-2 data\n",
    "logger.info(f\"Searching for {COLLECTION} data...\")\n",
    "search = catalog.search(\n",
    "    collections=[COLLECTION],\n",
    "    datetime=DATETIME,\n",
    "    bbox=BBOX,\n",
    "    limit=10,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 50}},  # Increased cloud cover threshold\n",
    ")\n",
    "\n",
    "# Get search results\n",
    "items = list(search.items())\n",
    "print(f\"üîç Found {len(items)} items with <50% cloud cover\")\n",
    "\n",
    "# If no items found, try with relaxed constraints\n",
    "if len(items) == 0:\n",
    "    print(\"‚ö†Ô∏è No items found, trying with relaxed constraints...\")\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION],\n",
    "        datetime=\"2023-06-01/2023-08-31\",  # Try summer period\n",
    "        bbox=BBOX,\n",
    "        limit=10,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "    )\n",
    "    items = list(search.items())\n",
    "    print(f\"üîç Found {len(items)} items with relaxed criteria\")\n",
    "\n",
    "if len(items) == 0:\n",
    "    raise ValueError(\"No suitable Sentinel-2 data found for the specified region and time period\")\n",
    "\n",
    "# Load data using odc-stac\n",
    "logger.info(\"Loading data with odc-stac...\")\n",
    "dataset = odc.stac.load(\n",
    "    items,\n",
    "    bands=BANDS,\n",
    "    resolution=100,  # 100m resolution for demo\n",
    "    chunks={\"time\": 1, \"x\": 512, \"y\": 512},\n",
    "    groupby=\"solar_day\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded dataset with shape: {dict(dataset.dims)}\")\n",
    "print(f\"üìä Data variables: {list(dataset.data_vars)}\")\n",
    "print(f\"‚è∞ Time range: {dataset.time.values[0]} to {dataset.time.values[-1]}\")\n",
    "\n",
    "# Display basic info\n",
    "_ = dataset  # Display dataset info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Create RGB Composite\n",
    "\n",
    "Create RGB composite for visualization and model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rgb_composite(dataset, time_index=-1):\n",
    "    \"\"\"Create RGB composite from dataset.\"\"\"\n",
    "    ds = dataset.isel(time=time_index) if \"time\" in dataset.dims else dataset\n",
    "\n",
    "    # Stack RGB bands\n",
    "    rgb = np.stack([ds.red, ds.green, ds.blue], axis=-1)\n",
    "\n",
    "    # Convert to reflectance (Sentinel-2 values are scaled by 10000)\n",
    "    rgb = rgb / 10000.0\n",
    "    rgb = np.clip(rgb, 0, 1)\n",
    "\n",
    "    return rgb\n",
    "\n",
    "\n",
    "# Create RGB composite from most recent image\n",
    "logger.info(\"Creating RGB composite...\")\n",
    "rgb_composite = create_rgb_composite(dataset, time_index=-1)\n",
    "\n",
    "print(f\"üì∏ RGB composite shape: {rgb_composite.shape}\")\n",
    "print(\n",
    "    f\"üìà Value range: [{np.nanmin(rgb_composite):.3f}, {np.nanmax(rgb_composite):.3f}]\"\n",
    ")\n",
    "\n",
    "# Visualize RGB composite\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(rgb_composite)\n",
    "plt.title(f\"RGB Composite - Auckland, New Zealand\\n{dataset.time.values[-1]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store for embedding generation\n",
    "rgb_array = rgb_composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Load TerraMind v1 Model\n",
    "\n",
    "Load IBM's TerraMind v1 foundation model for geospatial embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_terramind_availability():\n",
    "    \"\"\"Check if TerraMind models are available in current TerraTorch installation.\"\"\"\n",
    "    try:\n",
    "        available_models = list(BACKBONE_REGISTRY._registry.keys())\n",
    "        terramind_models = [name for name in available_models if 'terramind' in name.lower()]\n",
    "        \n",
    "        print(f\"üîç Found {len(terramind_models)} TerraMind models in registry:\")\n",
    "        for model in terramind_models:\n",
    "            print(f\"   ‚úÖ {model}\")\n",
    "            \n",
    "        return len(terramind_models) > 0, terramind_models\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking model registry: {e}\")\n",
    "        return False, []\n",
    "\n",
    "\n",
    "def load_terramind_model():\n",
    "    \"\"\"\n",
    "    Load TerraMind v1 model with the latest TerraTorch integration.\n",
    "    \n",
    "    Returns:\n",
    "        Loaded TerraMind model ready for inference\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # First check if TerraMind models are available\n",
    "    has_terramind, available_models = check_terramind_availability()\n",
    "    \n",
    "    if not has_terramind:\n",
    "        raise RuntimeError(\n",
    "            \"‚ùå TerraMind models not found in current TerraTorch installation.\\\\n\"\n",
    "            \"üîß Please install the latest TerraTorch from GitHub:\\\\n\"\n",
    "            \"   pip install 'git+https://github.com/terrastackai/terratorch.git'\\\\n\"\n",
    "            \"\\\\n\"\n",
    "            \"üìã Then restart your notebook kernel and try again.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Loading TerraMind v1 Base model...\")\n",
    "        print(\"ü§ñ Loading TerraMind v1 Base from HuggingFace...\")\n",
    "        print(\"üì• Downloading 1.52GB pretrained weights (first time only)...\")\n",
    "        \n",
    "        # Load TerraMind with working configuration\n",
    "        model = BACKBONE_REGISTRY.build(\n",
    "            'terratorch_terramind_v1_base',\n",
    "            modalities=['S2RGB'],  # 16x16 RGB patches\n",
    "            pretrained=True\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        logger.info(\"‚úÖ Successfully loaded TerraMind v1 Base\")\n",
    "        print(\"üéØ TerraMind v1 Base loaded successfully!\")\n",
    "        print(f\"üì± Device: {device}\")\n",
    "        print(\"üß† Embedding dimension: 768\")\n",
    "        print(\"üî≤ Patch size: 16x16 pixels\")\n",
    "        print(\"üåç Optimized for: Sentinel-2 RGB imagery\")\n",
    "        print(\"üîß Input modalities: S2RGB\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load TerraMind model: {e}\")\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"\\\\nüîß Troubleshooting steps:\")\n",
    "        print(\"1. Make sure you have the latest TerraTorch:\")\n",
    "        print(\"   pip install 'git+https://github.com/terrastackai/terratorch.git'\")\n",
    "        print(\"2. Restart your notebook kernel\")\n",
    "        print(\"3. Re-run the import cells\")\n",
    "        print(\"4. Check your internet connection for HuggingFace downloads\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def terramind_embeddings(model, patches_tensor):\n",
    "    \"\"\"\n",
    "    Generate TerraMind v1 embeddings from RGB patches.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded TerraMind model\n",
    "        patches_tensor: Tensor of shape [N, 3, 16, 16] (RGB patches)\n",
    "        \n",
    "    Returns:\n",
    "        embeddings: Array of shape [N, 768] (768-dimensional embeddings)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # TerraMind expects dictionary input with S2RGB key\n",
    "        outputs = model({'S2RGB': patches_tensor})\n",
    "        \n",
    "        # Use the last layer output (best representations)\n",
    "        embeddings = outputs[-1]  # Shape: [batch, 1, 768]\n",
    "        embeddings = embeddings.squeeze(1)  # Remove sequence dimension -> [batch, 768]\n",
    "        \n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "# Load TerraMind v1 model\n",
    "try:\n",
    "    print(\"üöÄ Loading TerraMind v1 foundation model...\")\n",
    "    model = load_terramind_model()\n",
    "    print(\"‚úÖ TerraMind v1 model ready for embedding generation!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading TerraMind: {e}\")\n",
    "    print(\"\\\\n‚ö†Ô∏è This notebook requires TerraMind v1 which is only available\")\n",
    "    print(\"   in the latest TerraTorch from GitHub. Please follow the\")\n",
    "    print(\"   installation instructions above.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for TerraMind v1\n",
    "\n",
    "Extract 16x16 patches and normalize for TerraMind model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_smooth_quantiles(rgb_array, quantiles=None):\n",
    "    \"\"\"Apply smooth quantile normalization to RGB data.\"\"\"\n",
    "    if quantiles is None:\n",
    "        quantiles = [0.02, 0.98]\n",
    "    \n",
    "    normalized = np.zeros_like(rgb_array)\n",
    "\n",
    "    for i in range(3):  # RGB channels\n",
    "        channel = rgb_array[:, :, i]\n",
    "        valid_mask = ~np.isnan(channel)\n",
    "\n",
    "        if valid_mask.any():\n",
    "            q_low, q_high = np.quantile(channel[valid_mask], quantiles)\n",
    "            normalized[:, :, i] = np.clip((channel - q_low) / (q_high - q_low), 0, 1)\n",
    "        else:\n",
    "            normalized[:, :, i] = channel\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def prepare_terramind_patches(rgb_data, patch_size=16):\n",
    "    \"\"\"\n",
    "    Extract 16x16 patches optimized for TerraMind v1.\n",
    "    \n",
    "    TerraMind v1 is designed specifically for 16x16 RGB patches from Sentinel-2 imagery.\n",
    "    We use non-overlapping patches for efficient processing.\n",
    "    \n",
    "    Args:\n",
    "        rgb_data: RGB image array [H, W, 3] in [0, 1] range\n",
    "        patch_size: Patch size (16 for TerraMind)\n",
    "        \n",
    "    Returns:\n",
    "        patches: Array of patches [N, 16, 16, 3] \n",
    "        coordinates: Patch coordinates for spatial reference\n",
    "    \"\"\"\n",
    "    height, width, channels = rgb_data.shape\n",
    "    patches = []\n",
    "    coordinates = []  # Store patch coordinates for spatial analysis\n",
    "    \n",
    "    print(f\"üéØ Extracting {patch_size}x{patch_size} patches for TerraMind v1\")\n",
    "    \n",
    "    # Use non-overlapping grid for efficient processing\n",
    "    for y in range(0, height - patch_size + 1, patch_size):\n",
    "        for x in range(0, width - patch_size + 1, patch_size):\n",
    "            patch = rgb_data[y : y + patch_size, x : x + patch_size, :]\n",
    "            \n",
    "            # Skip patches with too many NaN values\n",
    "            if np.isnan(patch).sum() / patch.size < 0.1:  # Less than 10% NaN\n",
    "                patches.append(patch)\n",
    "                coordinates.append((y, x))\n",
    "\n",
    "    print(f\"‚úÖ Extracted {len(patches)} valid patches\")\n",
    "    return np.array(patches), np.array(coordinates)\n",
    "\n",
    "\n",
    "def prepare_terramind_input(patches):\n",
    "    \"\"\"\n",
    "    Prepare patches for TerraMind v1 input.\n",
    "    \n",
    "    TerraMind expects:\n",
    "    - 16x16 RGB patches\n",
    "    - Values converted to [0, 255] range (like Sentinel-2 RGB)\n",
    "    - Standard ImageNet normalization applied\n",
    "    \n",
    "    Args:\n",
    "        patches: Array of patches [N, 16, 16, 3] in [0, 1] range\n",
    "        \n",
    "    Returns:\n",
    "        patches_tensor: Processed tensor [N, 3, 16, 16] ready for TerraMind\n",
    "    \"\"\"\n",
    "    # Convert to tensor and change to NCHW format\n",
    "    patches_tensor = torch.from_numpy(patches).float()\n",
    "    patches_tensor = patches_tensor.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "    \n",
    "    # Convert to [0, 255] range and apply ImageNet normalization \n",
    "    # (TerraMind expects this preprocessing)\n",
    "    patches_tensor = patches_tensor * 255.0\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    patches_tensor = (patches_tensor / 255.0 - mean) / std\n",
    "    \n",
    "    return patches_tensor\n",
    "\n",
    "\n",
    "# Extract and prepare patches for TerraMind\n",
    "print(\"üîÑ Preparing data for TerraMind v1...\")\n",
    "\n",
    "# Apply smooth normalization to RGB composite\n",
    "normalized_rgb = rgb_smooth_quantiles(rgb_composite)\n",
    "print(f\"üìä Normalized RGB shape: {normalized_rgb.shape}\")\n",
    "print(f\"üìà RGB value range: [{np.nanmin(normalized_rgb):.3f}, {np.nanmax(normalized_rgb):.3f}]\")\n",
    "\n",
    "# Extract 16x16 patches\n",
    "patches, coordinates = prepare_terramind_patches(normalized_rgb, patch_size=16)\n",
    "\n",
    "if len(patches) == 0:\n",
    "    raise ValueError(\"No valid patches extracted. Check input data.\")\n",
    "\n",
    "print(f\"üî≤ Patches shape: {patches.shape}\")\n",
    "print(f\"üìç Coordinate range: {coordinates.min(axis=0)} to {coordinates.max(axis=0)}\")\n",
    "\n",
    "# Prepare for TerraMind model\n",
    "patches_tensor = prepare_terramind_input(patches)\n",
    "print(f\"üéØ TerraMind input shape: {patches_tensor.shape}\")\n",
    "print(\"‚úÖ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Generate TerraMind v1 Embeddings\n",
    "\n",
    "Generate 768-dimensional embeddings from processed patches using TerraMind v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_terramind_embeddings_batch(patches_tensor, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate TerraMind v1 embeddings in batches.\n",
    "    \n",
    "    Args:\n",
    "        patches_tensor: Preprocessed patches [N, 3, 16, 16] \n",
    "        model: Loaded TerraMind v1 model\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        embeddings: Array [N, 768] of TerraMind embeddings\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    embeddings_list = []\n",
    "\n",
    "    print(f\"üß† Generating TerraMind v1 embeddings with batch size {batch_size}...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(patches_tensor), batch_size):\n",
    "            batch = patches_tensor[i : i + batch_size].to(device)\n",
    "\n",
    "            # TerraMind expects dictionary input with S2RGB key\n",
    "            outputs = model({'S2RGB': batch})\n",
    "            \n",
    "            # Use the last layer output (best representations)\n",
    "            batch_embeddings = outputs[-1]  # Shape: [batch, 1, 768]\n",
    "            batch_embeddings = batch_embeddings.squeeze(1)  # Remove sequence dim -> [batch, 768]\n",
    "\n",
    "            embeddings_list.append(batch_embeddings.cpu().numpy())\n",
    "\n",
    "            # Progress tracking every 10 batches\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"   Processed {i + len(batch)}/{len(patches_tensor)} patches\")\n",
    "\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} TerraMind v1 embeddings of dimension {embeddings.shape[1]}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Generate TerraMind v1 embeddings\n",
    "logger.info(\"Generating TerraMind v1 embeddings...\")\n",
    "embeddings = generate_terramind_embeddings_batch(patches_tensor, model, batch_size=16)\n",
    "\n",
    "print(f\"\\nüéØ TerraMind v1 Embedding Results:\")\n",
    "print(f\"   Shape: {embeddings.shape}\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"   Number of patches: {embeddings.shape[0]}\")\n",
    "\n",
    "print(f\"\\nüìä Embedding Statistics:\")\n",
    "print(f\"   Mean: {np.mean(embeddings):.4f}\")\n",
    "print(f\"   Std:  {np.std(embeddings):.4f}\")\n",
    "print(f\"   Min:  {np.min(embeddings):.4f}\")\n",
    "print(f\"   Max:  {np.max(embeddings):.4f}\")\n",
    "\n",
    "# Calculate embedding norms (magnitude analysis)\n",
    "embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(f\"   Mean L2 norm: {np.mean(embedding_norms):.4f}\")\n",
    "print(f\"   Std L2 norm: {np.std(embedding_norms):.4f}\")\n",
    "\n",
    "# Calculate cosine similarity for first 100 embeddings (sample)\n",
    "if len(embeddings) > 1:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    n_sample = min(100, len(embeddings))\n",
    "    sample_embeddings = embeddings[:n_sample]\n",
    "    similarity_matrix = cosine_similarity(sample_embeddings)\n",
    "    \n",
    "    # Remove diagonal (self-similarity) for meaningful average\n",
    "    mask = np.ones_like(similarity_matrix, dtype=bool)\n",
    "    np.fill_diagonal(mask, 0)\n",
    "    \n",
    "    avg_similarity = np.mean(similarity_matrix[mask])\n",
    "    print(f\"   Avg cosine similarity ({n_sample} samples): {avg_similarity:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ TerraMind v1 embedding generation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction\n",
    "\n",
    "Reduce embeddings to 3D for visualization using PCA and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample embeddings for visualization (if too many)\n",
    "n_vis = min(1000, len(embeddings))\n",
    "if n_vis < len(embeddings):\n",
    "    indices = np.random.choice(len(embeddings), n_vis, replace=False)\n",
    "    embeddings_vis = embeddings[indices]\n",
    "    print(f\"üìâ Subsampled {n_vis} embeddings for visualization\")\n",
    "else:\n",
    "    embeddings_vis = embeddings\n",
    "    indices = np.arange(len(embeddings))\n",
    "\n",
    "# Apply PCA for initial dimensionality reduction\n",
    "print(\"üîÑ Applying PCA...\")\n",
    "pca = PCA(n_components=50)  # Reduce to 50D first\n",
    "embeddings_pca = pca.fit_transform(embeddings_vis)\n",
    "print(\n",
    "    f\"üìä PCA explained variance ratio (first 5 components): {pca.explained_variance_ratio_[:5]}\"\n",
    ")\n",
    "print(\n",
    "    f\"üìà Total variance explained by 50 components: {pca.explained_variance_ratio_.sum():.3f}\"\n",
    ")\n",
    "\n",
    "# Apply t-SNE for 3D visualization\n",
    "print(\"üîÑ Applying t-SNE for 3D reduction...\")\n",
    "tsne = TSNE(\n",
    "    n_components=3, random_state=42, perplexity=min(30, len(embeddings_vis) - 1)\n",
    ")\n",
    "embeddings_3d = tsne.fit_transform(embeddings_pca)\n",
    "\n",
    "print(f\"‚úÖ Reduced to 3D: {embeddings_3d.shape}\")\n",
    "\n",
    "# Also create PCA 3D for comparison\n",
    "pca_3d = PCA(n_components=3)\n",
    "embeddings_pca_3d = pca_3d.fit_transform(embeddings_vis)\n",
    "\n",
    "print(f\"üìä PCA 3D explained variance: {pca_3d.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Calculate colors based on embedding magnitudes\n",
    "embedding_norms = np.linalg.norm(embeddings_vis, axis=1)\n",
    "colors = (embedding_norms - embedding_norms.min()) / (\n",
    "    embedding_norms.max() - embedding_norms.min()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 9. Interactive 3D Visualization with HoloViews\n",
    "\n",
    "Create interactive 3D scatter plots of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "def create_scatter_data(coords_3d, colors, method_name):\n",
    "    \"\"\"Create data dictionary for scatter plot.\"\"\"\n",
    "    return {\n",
    "        \"x\": coords_3d[:, 0],\n",
    "        \"y\": coords_3d[:, 1],\n",
    "        \"z\": coords_3d[:, 2] if coords_3d.shape[1] > 2 else coords_3d[:, 0],\n",
    "        \"color\": colors,\n",
    "        \"method\": [method_name] * len(coords_3d),\n",
    "        \"patch_id\": indices,\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "tsne_data = create_scatter_data(embeddings_3d, colors, \"t-SNE\")\n",
    "pca_data = create_scatter_data(embeddings_pca_3d, colors, \"PCA\")\n",
    "\n",
    "if HV_AVAILABLE:\n",
    "    # Create HoloViews 2D scatter plots (3D scatter may not be available)\n",
    "    opts_2d = {\n",
    "        \"width\": 600,\n",
    "        \"height\": 500,\n",
    "        \"color\": \"color\",\n",
    "        \"cmap\": \"viridis\",\n",
    "        \"size\": 4,\n",
    "        \"alpha\": 0.7,\n",
    "        \"colorbar\": True,\n",
    "        \"tools\": [\"hover\"],\n",
    "    }\n",
    "\n",
    "    # t-SNE plot\n",
    "    tsne_plot = hv.Scatter(\n",
    "        tsne_data, kdims=[\"x\", \"y\"], vdims=[\"color\", \"patch_id\"]\n",
    "    ).opts(title=\"t-SNE Embedding Space\", **opts_2d)\n",
    "\n",
    "    # PCA plot\n",
    "    pca_plot = hv.Scatter(\n",
    "        pca_data, kdims=[\"x\", \"y\"], vdims=[\"color\", \"patch_id\"]\n",
    "    ).opts(title=\"PCA Embedding Space\", **opts_2d)\n",
    "\n",
    "    print(\"üé® Created interactive scatter plots!\")\n",
    "    print(\"üí° Color represents embedding magnitude\")\n",
    "    print(\"üñ±Ô∏è Use mouse to zoom and explore\")\n",
    "\n",
    "    # Display plots side by side\n",
    "    layout = (tsne_plot + pca_plot).cols(2)\n",
    "    display(layout)  # Explicitly display instead of bare expression\n",
    "else:\n",
    "    # Fallback to matplotlib plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # t-SNE plot\n",
    "    scatter1 = axes[0].scatter(\n",
    "        embeddings_3d[:, 0], embeddings_3d[:, 1],\n",
    "        c=colors, cmap=\"viridis\", alpha=0.7, s=10\n",
    "    )\n",
    "    axes[0].set_title(\"t-SNE Embedding Space\")\n",
    "    axes[0].set_xlabel(\"Component 1\")\n",
    "    axes[0].set_ylabel(\"Component 2\")\n",
    "    plt.colorbar(scatter1, ax=axes[0])\n",
    "    \n",
    "    # PCA plot\n",
    "    scatter2 = axes[1].scatter(\n",
    "        embeddings_pca_3d[:, 0], embeddings_pca_3d[:, 1],\n",
    "        c=colors, cmap=\"viridis\", alpha=0.7, s=10\n",
    "    )\n",
    "    axes[1].set_title(\"PCA Embedding Space\")\n",
    "    axes[1].set_xlabel(\"PC 1\")\n",
    "    axes[1].set_ylabel(\"PC 2\")\n",
    "    plt.colorbar(scatter2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"üìä Created 2D visualization with matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 10. Advanced Embedding Analysis\n",
    "\n",
    "Analyze the structure and characteristics of the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding dimensions\n",
    "dim_means = np.mean(embeddings, axis=0)\n",
    "dim_stds = np.std(embeddings, axis=0)\n",
    "\n",
    "# Find most informative dimensions\n",
    "most_variable_dims = np.argsort(dim_stds)[-10:]\n",
    "highest_activation_dims = np.argsort(np.abs(dim_means))[-10:]\n",
    "\n",
    "print(\"üìä Embedding Analysis:\")\n",
    "print(f\"   Total dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"   Most variable dimensions: {most_variable_dims}\")\n",
    "print(f\"   Highest activation dimensions: {highest_activation_dims}\")\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Embedding magnitude distribution\n",
    "axes[0, 0].hist(embedding_norms, bins=50, alpha=0.7, color=\"skyblue\")\n",
    "axes[0, 0].set_title(\"Distribution of Embedding Magnitudes\")\n",
    "axes[0, 0].set_xlabel(\"L2 Norm\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Dimension variance plot\n",
    "axes[0, 1].plot(np.sort(dim_stds)[::-1], color=\"orange\")\n",
    "axes[0, 1].set_title(\"Dimension Standard Deviations (Sorted)\")\n",
    "axes[0, 1].set_xlabel(\"Dimension Rank\")\n",
    "axes[0, 1].set_ylabel(\"Standard Deviation\")\n",
    "axes[0, 1].set_yscale(\"log\")\n",
    "\n",
    "# Cosine similarity heatmap (subset)\n",
    "n_sample = min(50, len(embeddings))\n",
    "sample_indices = np.random.choice(len(embeddings), n_sample, replace=False)\n",
    "similarity_subset = cosine_similarity(embeddings[sample_indices])\n",
    "\n",
    "im = axes[1, 0].imshow(similarity_subset, cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "axes[1, 0].set_title(f\"Cosine Similarity Matrix ({n_sample} samples)\")\n",
    "axes[1, 0].set_xlabel(\"Patch Index\")\n",
    "axes[1, 0].set_ylabel(\"Patch Index\")\n",
    "plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "# Most variable dimensions\n",
    "axes[1, 1].bar(\n",
    "    range(len(most_variable_dims)),\n",
    "    dim_stds[most_variable_dims],\n",
    "    color=\"green\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "axes[1, 1].set_title(\"10 Most Variable Dimensions\")\n",
    "axes[1, 1].set_xlabel(\"Dimension Index\")\n",
    "axes[1, 1].set_ylabel(\"Standard Deviation\")\n",
    "axes[1, 1].set_xticks(range(len(most_variable_dims)))\n",
    "axes[1, 1].set_xticklabels(most_variable_dims, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüéØ Summary Statistics:\")\n",
    "print(f\"   Mean embedding magnitude: {np.mean(embedding_norms):.4f}\")\n",
    "print(f\"   Std embedding magnitude: {np.std(embedding_norms):.4f}\")\n",
    "print(f\"   Mean pairwise cosine similarity: {np.mean(similarity_subset):.4f}\")\n",
    "print(\n",
    "    f\"   Dimension with highest variance: {most_variable_dims[-1]} (œÉ={dim_stds[most_variable_dims[-1]]:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"   Dimension with highest activation: {highest_activation_dims[-1]} (Œº={dim_means[highest_activation_dims[-1]]:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 11. Save Results\n",
    "\n",
    "Save embeddings and visualization data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save TerraMind v1 embeddings\n",
    "embeddings_file = output_dir / \"notebook_terramind_v1_embeddings.npy\"\n",
    "np.save(embeddings_file, embeddings)\n",
    "\n",
    "# Save 3D coordinates for visualization\n",
    "np.save(output_dir / \"terramind_v1_tsne_3d.npy\", embeddings_3d)\n",
    "np.save(output_dir / \"terramind_v1_pca_3d.npy\", embeddings_pca_3d)\n",
    "\n",
    "# Save comprehensive metadata\n",
    "metadata = {\n",
    "    \"model\": \"terratorch_terramind_v1_base\",\n",
    "    \"model_description\": \"IBM TerraMind v1 Base - Geospatial Foundation Model\",\n",
    "    \"embedding_dimension\": 768,\n",
    "    \"patch_size\": 16,\n",
    "    \"num_patches\": len(embeddings),\n",
    "    \"original_image_shape\": rgb_array.shape,\n",
    "    \"area_description\": \"Auckland, New Zealand\",\n",
    "    \"bbox\": BBOX,\n",
    "    \"datetime\": DATETIME,\n",
    "    \"data_source\": \"Element84 Earth Search (Sentinel-2 L2A)\",\n",
    "    \"processing_details\": {\n",
    "        \"patch_extraction\": \"Non-overlapping 16x16 RGB patches\",\n",
    "        \"normalization\": \"ImageNet-style preprocessing for TerraMind\",\n",
    "        \"modalities\": [\"S2RGB\"],\n",
    "        \"device\": str(next(model.parameters()).device)\n",
    "    },\n",
    "    \"embedding_statistics\": {\n",
    "        \"mean\": float(np.mean(embeddings)),\n",
    "        \"std\": float(np.std(embeddings)),\n",
    "        \"min\": float(np.min(embeddings)),\n",
    "        \"max\": float(np.max(embeddings)),\n",
    "        \"mean_l2_norm\": float(np.mean(embedding_norms)),\n",
    "        \"std_l2_norm\": float(np.std(embedding_norms))\n",
    "    },\n",
    "    \"dimensionality_reduction\": {\n",
    "        \"pca_explained_variance_3d\": float(pca_3d.explained_variance_ratio_.sum()),\n",
    "        \"pca_explained_variance_50d\": float(pca.explained_variance_ratio_.sum()),\n",
    "        \"tsne_perplexity\": min(30, len(embeddings_vis) - 1)\n",
    "    },\n",
    "    \"similarity_analysis\": {\n",
    "        \"avg_cosine_similarity\": float(avg_similarity) if 'avg_similarity' in locals() else None,\n",
    "        \"sample_size\": n_sample if 'n_sample' in locals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / \"terramind_v1_notebook_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved TerraMind v1 results to {output_dir}:\")\n",
    "print(f\"   üß† embeddings: {embeddings_file}\")\n",
    "print(\"   üìä 3D coordinates: terramind_v1_tsne_3d.npy, terramind_v1_pca_3d.npy\") \n",
    "print(\"   üìÑ metadata: terramind_v1_notebook_metadata.json\")\n",
    "print(f\"\\nüéâ TerraMind v1 embedding generation completed successfully!\")\n",
    "print(f\"üìä Generated {len(embeddings)} embeddings from {len(patches)} patches\")\n",
    "print(\"üé® Interactive 3D visualization shows TerraMind embedding space structure\")\n",
    "print(f\"üéØ Average embedding magnitude: {np.mean(embedding_norms):.2f}\")\n",
    "print(f\"üîó Average cosine similarity: {avg_similarity:.3f}\" if 'avg_similarity' in locals() else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## üéâ TerraMind v1 Demo Complete!\n",
    "\n",
    "This notebook demonstrated the complete workflow for generating geospatial embeddings from satellite imagery using **IBM's TerraMind v1 foundation model**:\n",
    "\n",
    "### ‚úÖ What We Accomplished\n",
    "\n",
    "1. **üì° Connected to Element84 Earth Search** - Accessed cloud-native STAC catalog for satellite data\n",
    "2. **üõ∞Ô∏è Loaded Sentinel-2 imagery** - Used odc-stac for efficient multi-temporal data loading  \n",
    "3. **üñºÔ∏è Created RGB composites** - Processed satellite data into TerraMind-ready format\n",
    "4. **ü§ñ Loaded TerraMind v1 model** - IBM's state-of-the-art geospatial foundation model from HuggingFace\n",
    "5. **‚úÇÔ∏è Extracted 16x16 patches** - Prepared optimal patch size for TerraMind architecture\n",
    "6. **üß† Generated 768D embeddings** - Created high-dimensional geospatial representations\n",
    "7. **üìä Applied dimensionality reduction** - Used PCA and t-SNE for visualization\n",
    "8. **üé® Created 3D visualizations** - Interactive exploration of TerraMind embedding space\n",
    "\n",
    "### üéØ Key TerraMind v1 Insights\n",
    "\n",
    "- **Embedding Structure**: TerraMind's 768D embeddings capture rich geospatial patterns that cluster meaningfully in reduced space\n",
    "- **Patch Optimization**: 16x16 RGB patches provide optimal balance between spatial detail and computational efficiency\n",
    "- **Similarity Patterns**: Geospatially similar areas (water, vegetation, urban) cluster together in embedding space\n",
    "- **Foundation Model Power**: Pre-training on massive satellite datasets enables strong general representations\n",
    "- **Production Ready**: Successfully processed thousands of patches with consistent, high-quality embeddings\n",
    "\n",
    "### üöÄ TerraMind v1 Performance\n",
    "- **Model**: `terratorch_terramind_v1_base` from IBM/HuggingFace Hub\n",
    "- **Architecture**: Vision Transformer optimized for geospatial data\n",
    "- **Input**: 16x16 RGB patches from Sentinel-2 imagery\n",
    "- **Output**: 768-dimensional feature vectors\n",
    "- **Processing**: Batch inference with automatic GPU/CPU selection\n",
    "\n",
    "### üåç Next Steps for Geospatial ML\n",
    "\n",
    "- **Fine-tuning**: Adapt TerraMind for specific land cover classification tasks\n",
    "- **Time Series**: Apply TerraMind to multi-temporal change detection\n",
    "- **Scale Up**: Process entire regions using cloud computing resources\n",
    "- **Integration**: Embed TerraMind in operational monitoring workflows\n",
    "- **Research**: Explore TerraMind's learned representations for Earth science applications\n",
    "\n",
    "### üèÜ FOSS4G 2025 Demonstration\n",
    "\n",
    "This notebook showcases the cutting edge of **geospatial foundation models** integrated with **cloud-native data workflows**, demonstrating how modern AI can transform satellite imagery analysis at scale.\n",
    "\n",
    "**Ready to explore TerraMind embeddings for your geospatial applications!** üåçü§ñ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foss4g_2025_geofm",
   "language": "python",
   "name": "foss4g_2025_geofm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
