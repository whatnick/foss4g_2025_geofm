{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running for the first time)\n",
    "# %pip install holoviews bokeh scikit-learn\n",
    "\n",
    "# For this demo, we'll install the missing packages directly\n",
    "%uv pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# FOSS4G 2025 Demo: TerraMind Embedding Generation with odc-stac\n",
    "\n",
    "This notebook demonstrates the complete workflow for generating geospatial embeddings from satellite imagery:\n",
    "\n",
    "1. **Load satellite data** from STAC catalogs using odc-stac\n",
    "2. **Process RGB composites** for foundation model input\n",
    "3. **Load TerraMind model** (or fallback models) with TerraTorch\n",
    "4. **Generate embeddings** from 16x16 pixel patches\n",
    "5. **Visualize embeddings** in 3D space using dimensionality reduction\n",
    "\n",
    "## Key Technologies\n",
    "\n",
    "- **odc-stac**: Load STAC items into xarray Datasets\n",
    "- **TerraTorch**: Foundation model integration and training toolkit\n",
    "- **TerraMind**: IBM's geospatial foundation model (768-dim embeddings)\n",
    "- **Element84 Earth Search**: AWS-hosted STAC catalog for satellite data\n",
    "- **HoloViews**: Interactive 3D visualization of embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for our TerraMind embedding generation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running for the first time)\n",
    "# !pip install odc-stac terratorch pystac-client xarray rasterio matplotlib\n",
    "# !pip install holoviews bokeh scikit-learn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odc.stac\n",
    "\n",
    "# STAC and data loading\n",
    "import pystac_client\n",
    "\n",
    "# TerraTorch and ML\n",
    "try:\n",
    "    import torch\n",
    "    # Try multiple import patterns for BACKBONE_REGISTRY\n",
    "    try:\n",
    "        from terratorch.models.backbones import BACKBONE_REGISTRY\n",
    "        print(\"‚úÖ TerraTorch BACKBONE_REGISTRY imported successfully\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from terratorch import BACKBONE_REGISTRY\n",
    "            print(\"‚úÖ TerraTorch BACKBONE_REGISTRY imported from main module\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è BACKBONE_REGISTRY not found, will use fallback models\")\n",
    "            BACKBONE_REGISTRY = None\n",
    "    \n",
    "    print(\"‚úÖ TerraTorch imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è TerraTorch import issue: {e}\")\n",
    "    BACKBONE_REGISTRY = None\n",
    "\n",
    "# Visualization libraries\n",
    "try:\n",
    "    import holoviews as hv\n",
    "    hv.extension(\"bokeh\")\n",
    "    HV_AVAILABLE = True\n",
    "    print(\"‚úÖ HoloViews imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è HoloViews not available: {e}\")\n",
    "    print(\"üìä Will use matplotlib for visualization instead\")\n",
    "    HV_AVAILABLE = False\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üöÄ All available libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Connect to STAC Catalog\n",
    "\n",
    "Connect to Element84 Earth Search STAC catalog for satellite data discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "STAC_URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "COLLECTION = \"sentinel-2-l2a\"\n",
    "\n",
    "# Auckland, New Zealand - demo area\n",
    "BBOX = [174.6, -36.95, 174.85, -36.75]\n",
    "DATETIME = \"2023-12-01/2023-12-31\"\n",
    "BANDS = [\"red\", \"green\", \"blue\", \"nir\"]\n",
    "\n",
    "# Connect to STAC catalog\n",
    "logger.info(f\"Connecting to STAC catalog: {STAC_URL}\")\n",
    "catalog = pystac_client.Client.open(STAC_URL)\n",
    "print(f\"‚úÖ Connected to {catalog.title}\")\n",
    "\n",
    "# Display catalog information\n",
    "print(f\"üìç Catalog URL: {STAC_URL}\")\n",
    "print(f\"üóÇÔ∏è Available collections: {len(list(catalog.get_collections()))}\")\n",
    "print(f\"üéØ Target collection: {COLLECTION}\")\n",
    "print(f\"üì¶ Area of Interest: {BBOX} (Auckland, NZ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Search and Load Satellite Data\n",
    "\n",
    "Search for Sentinel-2 imagery and load it using odc-stac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Sentinel-2 data\n",
    "logger.info(f\"Searching for {COLLECTION} data...\")\n",
    "search = catalog.search(\n",
    "    collections=[COLLECTION],\n",
    "    datetime=DATETIME,\n",
    "    bbox=BBOX,\n",
    "    limit=10,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 50}},  # Increased cloud cover threshold\n",
    ")\n",
    "\n",
    "# Get search results\n",
    "items = list(search.items())\n",
    "print(f\"üîç Found {len(items)} items with <50% cloud cover\")\n",
    "\n",
    "# If no items found, try with relaxed constraints\n",
    "if len(items) == 0:\n",
    "    print(\"‚ö†Ô∏è No items found, trying with relaxed constraints...\")\n",
    "    search = catalog.search(\n",
    "        collections=[COLLECTION],\n",
    "        datetime=\"2023-06-01/2023-08-31\",  # Try summer period\n",
    "        bbox=BBOX,\n",
    "        limit=10,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    "    )\n",
    "    items = list(search.items())\n",
    "    print(f\"üîç Found {len(items)} items with relaxed criteria\")\n",
    "\n",
    "if len(items) == 0:\n",
    "    raise ValueError(\"No suitable Sentinel-2 data found for the specified region and time period\")\n",
    "\n",
    "# Load data using odc-stac\n",
    "logger.info(\"Loading data with odc-stac...\")\n",
    "dataset = odc.stac.load(\n",
    "    items,\n",
    "    bands=BANDS,\n",
    "    resolution=100,  # 100m resolution for demo\n",
    "    chunks={\"time\": 1, \"x\": 512, \"y\": 512},\n",
    "    groupby=\"solar_day\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded dataset with shape: {dict(dataset.dims)}\")\n",
    "print(f\"üìä Data variables: {list(dataset.data_vars)}\")\n",
    "print(f\"‚è∞ Time range: {dataset.time.values[0]} to {dataset.time.values[-1]}\")\n",
    "\n",
    "# Display basic info\n",
    "_ = dataset  # Display dataset info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Create RGB Composite\n",
    "\n",
    "Create RGB composite for visualization and model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rgb_composite(dataset, time_index=-1):\n",
    "    \"\"\"Create RGB composite from dataset.\"\"\"\n",
    "    ds = dataset.isel(time=time_index) if \"time\" in dataset.dims else dataset\n",
    "\n",
    "    # Stack RGB bands\n",
    "    rgb = np.stack([ds.red, ds.green, ds.blue], axis=-1)\n",
    "\n",
    "    # Convert to reflectance (Sentinel-2 values are scaled by 10000)\n",
    "    rgb = rgb / 10000.0\n",
    "    rgb = np.clip(rgb, 0, 1)\n",
    "\n",
    "    return rgb\n",
    "\n",
    "\n",
    "# Create RGB composite from most recent image\n",
    "logger.info(\"Creating RGB composite...\")\n",
    "rgb_composite = create_rgb_composite(dataset, time_index=-1)\n",
    "\n",
    "print(f\"üì∏ RGB composite shape: {rgb_composite.shape}\")\n",
    "print(\n",
    "    f\"üìà Value range: [{np.nanmin(rgb_composite):.3f}, {np.nanmax(rgb_composite):.3f}]\"\n",
    ")\n",
    "\n",
    "# Visualize RGB composite\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(rgb_composite)\n",
    "plt.title(f\"RGB Composite - Auckland, New Zealand\\n{dataset.time.values[-1]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store for embedding generation\n",
    "rgb_array = rgb_composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Load TerraMind Model\n",
    "\n",
    "Load TerraMind foundation model with robust fallback system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_terramind_model():\n",
    "    \"\"\"Load TerraMind model with fallback system.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Model fallback chain\n",
    "    models_to_try = [\n",
    "        (\"terramind_v1_base\", \"TerraMind foundation model\"),\n",
    "        (\"clay_v1\", \"Clay foundation model\"),\n",
    "        (\"prithvi_vit\", \"Prithvi Vision Transformer\"),\n",
    "        (\"resnet18\", \"ResNet18 (timm)\"),\n",
    "    ]\n",
    "\n",
    "    for model_name, description in models_to_try:\n",
    "        try:\n",
    "            logger.info(f\"Attempting to load model: {model_name}\")\n",
    "\n",
    "            if model_name == \"resnet18\":\n",
    "                # Special handling for timm models\n",
    "                import timm\n",
    "                model = timm.create_model(\"resnet18\", pretrained=True, num_classes=0)\n",
    "            else:\n",
    "                # TerraTorch models\n",
    "                if BACKBONE_REGISTRY is None:\n",
    "                    raise ImportError(\"BACKBONE_REGISTRY not available\")\n",
    "                \n",
    "                model = BACKBONE_REGISTRY.build(\n",
    "                    model_name,\n",
    "                    modalities=[\"S2RGB\"] if \"terra\" in model_name or \"clay\" in model_name else None,\n",
    "                    pretrained=True,\n",
    "                )\n",
    "\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            logger.info(f\"‚úÖ Successfully loaded {model_name}\")\n",
    "            print(f\"ü§ñ Model: {description}\")\n",
    "            print(f\"üì± Device: {device}\")\n",
    "\n",
    "            if model_name != \"terramind_v1_base\":\n",
    "                print(\"‚ö†Ô∏è TerraMind not available, using fallback model\")\n",
    "                print(\"‚ö†Ô∏è Embeddings will be generated but may not be TerraMind-specific\")\n",
    "\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Failed to load {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\"Could not load any model from the fallback chain\")\n",
    "\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = load_terramind_model()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"‚ö†Ô∏è Please ensure terratorch is installed: pip install terratorch\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for TerraMind\n",
    "\n",
    "Extract patches and normalize for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_smooth_quantiles(rgb_array, quantiles=None):\n",
    "    \"\"\"Apply smooth quantile normalization to RGB data.\"\"\"\n",
    "    if quantiles is None:\n",
    "        quantiles = [0.02, 0.98]\n",
    "    \n",
    "    normalized = np.zeros_like(rgb_array)\n",
    "\n",
    "    for i in range(3):  # RGB channels\n",
    "        channel = rgb_array[:, :, i]\n",
    "        valid_mask = ~np.isnan(channel)\n",
    "\n",
    "        if valid_mask.any():\n",
    "            q_low, q_high = np.quantile(channel[valid_mask], quantiles)\n",
    "            normalized[:, :, i] = np.clip((channel - q_low) / (q_high - q_low), 0, 1)\n",
    "        else:\n",
    "            normalized[:, :, i] = channel\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def prepare_terramind_patches(rgb_data, patch_size=16):\n",
    "    \"\"\"Extract 16x16 patches from RGB data.\"\"\"\n",
    "    height, width, channels = rgb_data.shape\n",
    "    patches = []\n",
    "\n",
    "    for y in range(0, height - patch_size + 1, patch_size):\n",
    "        for x in range(0, width - patch_size + 1, patch_size):\n",
    "            patch = rgb_data[y : y + patch_size, x : x + patch_size, :]\n",
    "            if not np.isnan(patch).any():  # Skip patches with NaN values\n",
    "                patches.append(patch)\n",
    "\n",
    "    return np.array(patches)\n",
    "\n",
    "\n",
    "def normalize_terramind_input(patches):\n",
    "    \"\"\"Normalize patches for model input.\"\"\"\n",
    "    # ImageNet normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    patches_tensor = torch.from_numpy(patches).float()\n",
    "    patches_tensor = patches_tensor.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "\n",
    "    for i in range(3):\n",
    "        patches_tensor[:, i] = (patches_tensor[:, i] - mean[i]) / std[i]\n",
    "\n",
    "    return patches_tensor\n",
    "\n",
    "\n",
    "# Process data\n",
    "logger.info(\"Applying smooth quantile normalization...\")\n",
    "normalized_rgb = rgb_smooth_quantiles(rgb_array)\n",
    "\n",
    "logger.info(\"Extracting 16x16 patches...\")\n",
    "patches = prepare_terramind_patches(normalized_rgb, patch_size=16)\n",
    "print(f\"üß© Extracted {len(patches)} patches\")\n",
    "\n",
    "logger.info(\"Normalizing patches for model input...\")\n",
    "patches_tensor = normalize_terramind_input(patches)\n",
    "print(f\"üì¶ Normalized tensor shape: {patches_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Generate Embeddings\n",
    "\n",
    "Generate embeddings from processed patches using the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(patches_tensor, model, batch_size=32):\n",
    "    \"\"\"Generate embeddings using the loaded model.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    embeddings_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(patches_tensor), batch_size):\n",
    "            batch = patches_tensor[i : i + batch_size].to(device)\n",
    "\n",
    "            try:\n",
    "                # Try TerraMind format first\n",
    "                batch_embeddings = model({\"S2RGB\": batch})\n",
    "            except (TypeError, KeyError, RuntimeError):\n",
    "                # Fall back to standard tensor input\n",
    "                try:\n",
    "                    batch_embeddings = model(batch)\n",
    "\n",
    "                    # Handle different return types\n",
    "                    if isinstance(batch_embeddings, list):\n",
    "                        batch_embeddings = batch_embeddings[-1]\n",
    "                    elif isinstance(batch_embeddings, tuple):\n",
    "                        batch_embeddings = batch_embeddings[0]\n",
    "                except Exception:\n",
    "                    # Last resort: try features extraction\n",
    "                    if hasattr(model, \"forward_features\"):\n",
    "                        batch_embeddings = model.forward_features(batch)\n",
    "                    elif hasattr(model, \"features\"):\n",
    "                        features = model.features(batch)\n",
    "                        batch_embeddings = torch.nn.functional.adaptive_avg_pool2d(\n",
    "                            features, (1, 1)\n",
    "                        ).flatten(1)\n",
    "                    else:\n",
    "                        raise Exception(\"Cannot extract embeddings from this model\")\n",
    "\n",
    "            # Ensure 2D embeddings\n",
    "            if hasattr(batch_embeddings, \"dim\") and batch_embeddings.dim() > 2:\n",
    "                spatial_dims = tuple(range(2, batch_embeddings.dim()))\n",
    "                batch_embeddings = torch.mean(batch_embeddings, dim=spatial_dims)\n",
    "\n",
    "            embeddings_list.append(batch_embeddings.cpu().numpy())\n",
    "\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + len(batch)}/{len(patches_tensor)} patches\")\n",
    "\n",
    "    return np.vstack(embeddings_list)\n",
    "\n",
    "\n",
    "# Generate embeddings\n",
    "logger.info(\"Generating embeddings...\")\n",
    "embeddings = generate_embeddings(patches_tensor, model, batch_size=16)\n",
    "\n",
    "print(f\"üéØ Generated embeddings shape: {embeddings.shape}\")\n",
    "print(\"üìä Embedding statistics:\")\n",
    "print(f\"   Mean: {np.mean(embeddings):.4f}\")\n",
    "print(f\"   Std:  {np.std(embeddings):.4f}\")\n",
    "print(f\"   Min:  {np.min(embeddings):.4f}\")\n",
    "print(f\"   Max:  {np.max(embeddings):.4f}\")\n",
    "\n",
    "# Calculate cosine similarity between first 10 embeddings\n",
    "if len(embeddings) > 1:\n",
    "    similarity_matrix = cosine_similarity(embeddings[:10])\n",
    "    avg_similarity = np.mean(similarity_matrix)\n",
    "    print(f\"   Avg cosine similarity (first 10): {avg_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction\n",
    "\n",
    "Reduce embeddings to 3D for visualization using PCA and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample embeddings for visualization (if too many)\n",
    "n_vis = min(1000, len(embeddings))\n",
    "if n_vis < len(embeddings):\n",
    "    indices = np.random.choice(len(embeddings), n_vis, replace=False)\n",
    "    embeddings_vis = embeddings[indices]\n",
    "    print(f\"üìâ Subsampled {n_vis} embeddings for visualization\")\n",
    "else:\n",
    "    embeddings_vis = embeddings\n",
    "    indices = np.arange(len(embeddings))\n",
    "\n",
    "# Apply PCA for initial dimensionality reduction\n",
    "print(\"üîÑ Applying PCA...\")\n",
    "pca = PCA(n_components=50)  # Reduce to 50D first\n",
    "embeddings_pca = pca.fit_transform(embeddings_vis)\n",
    "print(\n",
    "    f\"üìä PCA explained variance ratio (first 5 components): {pca.explained_variance_ratio_[:5]}\"\n",
    ")\n",
    "print(\n",
    "    f\"üìà Total variance explained by 50 components: {pca.explained_variance_ratio_.sum():.3f}\"\n",
    ")\n",
    "\n",
    "# Apply t-SNE for 3D visualization\n",
    "print(\"üîÑ Applying t-SNE for 3D reduction...\")\n",
    "tsne = TSNE(\n",
    "    n_components=3, random_state=42, perplexity=min(30, len(embeddings_vis) - 1)\n",
    ")\n",
    "embeddings_3d = tsne.fit_transform(embeddings_pca)\n",
    "\n",
    "print(f\"‚úÖ Reduced to 3D: {embeddings_3d.shape}\")\n",
    "\n",
    "# Also create PCA 3D for comparison\n",
    "pca_3d = PCA(n_components=3)\n",
    "embeddings_pca_3d = pca_3d.fit_transform(embeddings_vis)\n",
    "\n",
    "print(f\"üìä PCA 3D explained variance: {pca_3d.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Calculate colors based on embedding magnitudes\n",
    "embedding_norms = np.linalg.norm(embeddings_vis, axis=1)\n",
    "colors = (embedding_norms - embedding_norms.min()) / (\n",
    "    embedding_norms.max() - embedding_norms.min()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 9. Interactive 3D Visualization with HoloViews\n",
    "\n",
    "Create interactive 3D scatter plots of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "def create_scatter_data(coords_3d, colors, method_name):\n",
    "    \"\"\"Create data dictionary for scatter plot.\"\"\"\n",
    "    return {\n",
    "        \"x\": coords_3d[:, 0],\n",
    "        \"y\": coords_3d[:, 1],\n",
    "        \"z\": coords_3d[:, 2] if coords_3d.shape[1] > 2 else coords_3d[:, 0],\n",
    "        \"color\": colors,\n",
    "        \"method\": [method_name] * len(coords_3d),\n",
    "        \"patch_id\": indices,\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "tsne_data = create_scatter_data(embeddings_3d, colors, \"t-SNE\")\n",
    "pca_data = create_scatter_data(embeddings_pca_3d, colors, \"PCA\")\n",
    "\n",
    "if HV_AVAILABLE:\n",
    "    # Create HoloViews 2D scatter plots (3D scatter may not be available)\n",
    "    opts_2d = {\n",
    "        \"width\": 600,\n",
    "        \"height\": 500,\n",
    "        \"color\": \"color\",\n",
    "        \"cmap\": \"viridis\",\n",
    "        \"size\": 4,\n",
    "        \"alpha\": 0.7,\n",
    "        \"colorbar\": True,\n",
    "        \"tools\": [\"hover\"],\n",
    "    }\n",
    "\n",
    "    # t-SNE plot\n",
    "    tsne_plot = hv.Scatter(\n",
    "        tsne_data, kdims=[\"x\", \"y\"], vdims=[\"color\", \"patch_id\"]\n",
    "    ).opts(title=\"t-SNE Embedding Space\", **opts_2d)\n",
    "\n",
    "    # PCA plot\n",
    "    pca_plot = hv.Scatter(\n",
    "        pca_data, kdims=[\"x\", \"y\"], vdims=[\"color\", \"patch_id\"]\n",
    "    ).opts(title=\"PCA Embedding Space\", **opts_2d)\n",
    "\n",
    "    print(\"üé® Created interactive scatter plots!\")\n",
    "    print(\"üí° Color represents embedding magnitude\")\n",
    "    print(\"üñ±Ô∏è Use mouse to zoom and explore\")\n",
    "\n",
    "    # Display plots side by side\n",
    "    layout = (tsne_plot + pca_plot).cols(2)\n",
    "    display(layout)  # Explicitly display instead of bare expression\n",
    "else:\n",
    "    # Fallback to matplotlib plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # t-SNE plot\n",
    "    scatter1 = axes[0].scatter(\n",
    "        embeddings_3d[:, 0], embeddings_3d[:, 1],\n",
    "        c=colors, cmap=\"viridis\", alpha=0.7, s=10\n",
    "    )\n",
    "    axes[0].set_title(\"t-SNE Embedding Space\")\n",
    "    axes[0].set_xlabel(\"Component 1\")\n",
    "    axes[0].set_ylabel(\"Component 2\")\n",
    "    plt.colorbar(scatter1, ax=axes[0])\n",
    "    \n",
    "    # PCA plot\n",
    "    scatter2 = axes[1].scatter(\n",
    "        embeddings_pca_3d[:, 0], embeddings_pca_3d[:, 1],\n",
    "        c=colors, cmap=\"viridis\", alpha=0.7, s=10\n",
    "    )\n",
    "    axes[1].set_title(\"PCA Embedding Space\")\n",
    "    axes[1].set_xlabel(\"PC 1\")\n",
    "    axes[1].set_ylabel(\"PC 2\")\n",
    "    plt.colorbar(scatter2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"üìä Created 2D visualization with matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 10. Advanced Embedding Analysis\n",
    "\n",
    "Analyze the structure and characteristics of the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding dimensions\n",
    "dim_means = np.mean(embeddings, axis=0)\n",
    "dim_stds = np.std(embeddings, axis=0)\n",
    "\n",
    "# Find most informative dimensions\n",
    "most_variable_dims = np.argsort(dim_stds)[-10:]\n",
    "highest_activation_dims = np.argsort(np.abs(dim_means))[-10:]\n",
    "\n",
    "print(\"üìä Embedding Analysis:\")\n",
    "print(f\"   Total dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"   Most variable dimensions: {most_variable_dims}\")\n",
    "print(f\"   Highest activation dimensions: {highest_activation_dims}\")\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Embedding magnitude distribution\n",
    "axes[0, 0].hist(embedding_norms, bins=50, alpha=0.7, color=\"skyblue\")\n",
    "axes[0, 0].set_title(\"Distribution of Embedding Magnitudes\")\n",
    "axes[0, 0].set_xlabel(\"L2 Norm\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Dimension variance plot\n",
    "axes[0, 1].plot(np.sort(dim_stds)[::-1], color=\"orange\")\n",
    "axes[0, 1].set_title(\"Dimension Standard Deviations (Sorted)\")\n",
    "axes[0, 1].set_xlabel(\"Dimension Rank\")\n",
    "axes[0, 1].set_ylabel(\"Standard Deviation\")\n",
    "axes[0, 1].set_yscale(\"log\")\n",
    "\n",
    "# Cosine similarity heatmap (subset)\n",
    "n_sample = min(50, len(embeddings))\n",
    "sample_indices = np.random.choice(len(embeddings), n_sample, replace=False)\n",
    "similarity_subset = cosine_similarity(embeddings[sample_indices])\n",
    "\n",
    "im = axes[1, 0].imshow(similarity_subset, cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "axes[1, 0].set_title(f\"Cosine Similarity Matrix ({n_sample} samples)\")\n",
    "axes[1, 0].set_xlabel(\"Patch Index\")\n",
    "axes[1, 0].set_ylabel(\"Patch Index\")\n",
    "plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "# Most variable dimensions\n",
    "axes[1, 1].bar(\n",
    "    range(len(most_variable_dims)),\n",
    "    dim_stds[most_variable_dims],\n",
    "    color=\"green\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "axes[1, 1].set_title(\"10 Most Variable Dimensions\")\n",
    "axes[1, 1].set_xlabel(\"Dimension Index\")\n",
    "axes[1, 1].set_ylabel(\"Standard Deviation\")\n",
    "axes[1, 1].set_xticks(range(len(most_variable_dims)))\n",
    "axes[1, 1].set_xticklabels(most_variable_dims, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüéØ Summary Statistics:\")\n",
    "print(f\"   Mean embedding magnitude: {np.mean(embedding_norms):.4f}\")\n",
    "print(f\"   Std embedding magnitude: {np.std(embedding_norms):.4f}\")\n",
    "print(f\"   Mean pairwise cosine similarity: {np.mean(similarity_subset):.4f}\")\n",
    "print(\n",
    "    f\"   Dimension with highest variance: {most_variable_dims[-1]} (œÉ={dim_stds[most_variable_dims[-1]]:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"   Dimension with highest activation: {highest_activation_dims[-1]} (Œº={dim_means[highest_activation_dims[-1]]:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 11. Save Results\n",
    "\n",
    "Save embeddings and visualization data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"../outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_file = output_dir / \"notebook_embeddings.npy\"\n",
    "np.save(embeddings_file, embeddings)\n",
    "\n",
    "# Save 3D coordinates\n",
    "np.save(output_dir / \"embeddings_tsne_3d.npy\", embeddings_3d)\n",
    "np.save(output_dir / \"embeddings_pca_3d.npy\", embeddings_pca_3d)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"num_patches\": len(embeddings),\n",
    "    \"embedding_dim\": embeddings.shape[1],\n",
    "    \"original_image_shape\": rgb_array.shape,\n",
    "    \"patch_size\": 16,\n",
    "    \"area\": \"Auckland, New Zealand\",\n",
    "    \"bbox\": BBOX,\n",
    "    \"datetime\": DATETIME,\n",
    "    \"model_type\": type(model).__name__,\n",
    "    \"statistics\": {\n",
    "        \"mean\": float(np.mean(embeddings)),\n",
    "        \"std\": float(np.std(embeddings)),\n",
    "        \"min\": float(np.min(embeddings)),\n",
    "        \"max\": float(np.max(embeddings)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(output_dir / \"notebook_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved results to {output_dir}:\")\n",
    "print(f\"   üìÅ embeddings: {embeddings_file}\")\n",
    "print(\"   üìÅ 3D coordinates: embeddings_tsne_3d.npy, embeddings_pca_3d.npy\")\n",
    "print(\"   üìÅ metadata: notebook_metadata.json\")\n",
    "print(\"\\nüéâ TerraMind embedding generation completed successfully!\")\n",
    "print(f\"üìä Generated {len(embeddings)} embeddings from {len(patches)} patches\")\n",
    "print(\"üé® Interactive 3D visualization shows embedding space structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## üéâ Demo Complete!\n",
    "\n",
    "This notebook demonstrated the complete workflow for generating geospatial embeddings from satellite imagery:\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **üì° Connected to Element84 Earth Search** - Accessed cloud-native STAC catalog\n",
    "2. **üõ∞Ô∏è Loaded Sentinel-2 imagery** - Used odc-stac for efficient data loading  \n",
    "3. **üñºÔ∏è Created RGB composites** - Processed satellite data for model input\n",
    "4. **ü§ñ Loaded foundation models** - Used TerraTorch with robust fallback system\n",
    "5. **‚úÇÔ∏è Extracted image patches** - Prepared 16x16 pixel patches for embedding generation\n",
    "6. **üß† Generated embeddings** - Created high-dimensional feature representations\n",
    "7. **üìä Applied dimensionality reduction** - Used PCA and t-SNE for visualization\n",
    "8. **üé® Created 3D visualizations** - Interactive exploration of embedding space\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Embedding Structure**: The 3D visualizations reveal the underlying structure in how the foundation model represents different image patches\n",
    "- **Similarity Patterns**: Patches with similar visual characteristics cluster together in embedding space\n",
    "- **Dimensionality**: Foundation models capture rich representations that can be effectively reduced for visualization\n",
    "- **Geospatial Context**: The embeddings preserve spatial relationships and land cover patterns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Classification**: Use embeddings for land cover classification tasks\n",
    "- **Change Detection**: Compare embeddings across time periods\n",
    "- **Similarity Search**: Find similar landscape patterns across different regions\n",
    "- **Model Training**: Fine-tune foundation models using these embeddings as features\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [odc-stac Documentation](https://github.com/opendatacube/odc-stac)\n",
    "- [TerraTorch GitHub](https://github.com/IBM/terratorch)\n",
    "- [Element84 Earth Search](https://github.com/element84/earth-search)\n",
    "- [HoloViews Documentation](https://holoviews.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foss4g_2025_geofm",
   "language": "python",
   "name": "foss4g_2025_geofm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
