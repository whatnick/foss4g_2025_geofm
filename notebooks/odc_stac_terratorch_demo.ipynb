{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de0aa70",
   "metadata": {},
   "source": [
    "# FOSS4G 2025 Demo: Loading Geospatial Data via odc-stac into TerraTorch\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Connect to STAC catalogs for geospatial data discovery\n",
    "- Load satellite imagery using odc-stac into xarray Datasets\n",
    "- Convert data to TerraTorch format for machine learning workflows\n",
    "- Visualize and explore the loaded data\n",
    "- Prepare data for geospatial foundation model training\n",
    "\n",
    "## Overview\n",
    "\n",
    "**odc-stac** is a powerful library for loading STAC (SpatioTemporal Asset Catalog) items into xarray Datasets, while **TerraTorch** provides tools for fine-tuning geospatial foundation models. This integration enables efficient cloud-native geospatial ML workflows.\n",
    "\n",
    "**Target Use Cases:**\n",
    "- Multi-temporal satellite image analysis\n",
    "- Land cover classification and change detection\n",
    "- Environmental monitoring and assessment\n",
    "- Geospatial foundation model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c22bb",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install and import all necessary libraries for our geospatial data processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1121c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running for the first time)\n",
    "# !pip install odc-stac terratorch pystac-client xarray rasterio matplotlib cartopy dask\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries for geospatial data processing\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pathlib import Path\n",
    "\n",
    "# STAC and data loading libraries\n",
    "import pystac_client\n",
    "import odc.stac\n",
    "\n",
    "# TerraTorch and ML libraries\n",
    "try:\n",
    "    import terratorch\n",
    "    from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "    print(\"TerraTorch imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"TerraTorch not available. Install with: pip install terratorch\")\n",
    "\n",
    "# Check library versions\n",
    "print(f\"odc-stac version: {odc.stac.__version__}\")\n",
    "print(f\"pystac-client version: {pystac_client.__version__}\")\n",
    "print(f\"xarray version: {xr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec18dbe",
   "metadata": {},
   "source": [
    "## 2. Configure STAC Catalog Connection\n",
    "\n",
    "We'll connect to Element84's Earth Search STAC catalog, which provides free access to a vast collection of geospatial data including Sentinel-2, Landsat, and other Earth observation datasets hosted on AWS Open Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure STAC catalog connection\n",
    "STAC_URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "\n",
    "# Connect to the STAC catalog\n",
    "catalog = pystac_client.Client.open(STAC_URL)\n",
    "\n",
    "print(f\"Connected to STAC catalog: {STAC_URL}\")\n",
    "print(f\"Catalog ID: {catalog.id}\")\n",
    "print(f\"Catalog Description: {catalog.description}\")\n",
    "\n",
    "# List available collections\n",
    "collections = list(catalog.get_collections())\n",
    "print(f\"\\nAvailable collections ({len(collections)}):\")\n",
    "for collection in collections[:10]:  # Show first 10\n",
    "    print(f\"  - {collection.id}: {collection.title}\")\n",
    "if len(collections) > 10:\n",
    "    print(f\"  ... and {len(collections) - 10} more collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1523c39",
   "metadata": {},
   "source": [
    "## 3. Search and Discover STAC Items\n",
    "\n",
    "Now let's search for Sentinel-2 imagery over a specific area of interest. We'll focus on the San Francisco Bay Area for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92358a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define area of interest (San Francisco Bay Area)\n",
    "bbox = [-122.5, 37.4, -121.8, 38.0]  # [min_lon, min_lat, max_lon, max_lat]\n",
    "\n",
    "# Define time range\n",
    "start_date = \"2023-07-01\"\n",
    "end_date = \"2023-07-31\"\n",
    "datetime_range = f\"{start_date}/{end_date}\"\n",
    "\n",
    "print(f\"Search Parameters:\")\n",
    "print(f\"  Area: {bbox}\")\n",
    "print(f\"  Time Range: {datetime_range}\")\n",
    "print(f\"  Collection: sentinel-2-l2a\")\n",
    "\n",
    "# Search for STAC items\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime_range,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 20}}  # Less than 20% cloud cover\n",
    ")\n",
    "\n",
    "# Get items from search results\n",
    "items = list(search.items())\n",
    "print(f\"\\nFound {len(items)} Sentinel-2 items\")\n",
    "\n",
    "# Display information about the first few items\n",
    "for i, item in enumerate(items[:3]):\n",
    "    print(f\"\\nItem {i+1}:\")\n",
    "    print(f\"  ID: {item.id}\")\n",
    "    print(f\"  Date: {item.datetime}\")\n",
    "    print(f\"  Cloud Cover: {item.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
    "    print(f\"  Assets: {list(item.assets.keys())}\")\n",
    "    print(f\"  Geometry: {item.geometry['type']} with {len(item.geometry['coordinates'][0])} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450dd817",
   "metadata": {},
   "source": [
    "## 4. Load Data using odc-stac\n",
    "\n",
    "Now we'll use odc-stac to load the discovered STAC items into an xarray Dataset. This is where the magic happens - converting cloud-optimized geotiffs into analysis-ready data arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96227ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bands to load (Sentinel-2 10m resolution bands)\n",
    "bands = [\"red\", \"green\", \"blue\", \"nir\"]\n",
    "\n",
    "# Configure loading parameters\n",
    "load_params = {\n",
    "    \"bands\": bands,\n",
    "    \"resolution\": 60,  # 60m resolution for faster loading (can be 10, 20, or 60)\n",
    "    \"chunks\": {\"time\": 1, \"x\": 512, \"y\": 512},  # Chunking for memory efficiency\n",
    "    \"groupby\": \"solar_day\",  # Group by solar day to handle overlapping scenes\n",
    "}\n",
    "\n",
    "print(\"Loading parameters:\")\n",
    "for key, value in load_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Load data using odc-stac\n",
    "print(f\"\\nLoading {len(items)} STAC items...\")\n",
    "dataset = odc.stac.load(items, **load_params)\n",
    "\n",
    "print(f\"Successfully loaded dataset!\")\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Dimensions: {dict(dataset.dims)}\")\n",
    "print(f\"  Coordinates: {list(dataset.coords.keys())}\")\n",
    "print(f\"  Data variables: {list(dataset.data_vars.keys())}\")\n",
    "print(f\"  Size in memory: {dataset.nbytes / 1e6:.2f} MB\")\n",
    "\n",
    "# Display the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b93377d",
   "metadata": {},
   "source": [
    "## 5. Convert to TerraTorch Format\n",
    "\n",
    "TerraTorch expects data in specific formats. Let's prepare our xarray Dataset for use with TerraTorch's data modules and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_terratorch(dataset: xr.Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prepare xarray Dataset for TerraTorch compatibility.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset loaded from odc-stac\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prepared data and metadata\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays and handle data types\n",
    "    prepared_data = {}\n",
    "    \n",
    "    for band in dataset.data_vars:\n",
    "        # Convert to float32 for ML compatibility\n",
    "        data = dataset[band].astype(np.float32)\n",
    "        \n",
    "        # Normalize reflectance values (Sentinel-2 values are typically 0-10000)\n",
    "        data = data / 10000.0\n",
    "        \n",
    "        # Clip values to reasonable range\n",
    "        data = np.clip(data, 0, 1)\n",
    "        \n",
    "        prepared_data[band] = data\n",
    "    \n",
    "    # Metadata for TerraTorch\n",
    "    metadata = {\n",
    "        \"bands\": list(dataset.data_vars.keys()),\n",
    "        \"spatial_dims\": {\"x\": len(dataset.x), \"y\": len(dataset.y)},\n",
    "        \"temporal_dim\": len(dataset.time) if \"time\" in dataset.dims else 1,\n",
    "        \"crs\": str(dataset.spatial_ref.attrs.get(\"crs_wkt\", \"Unknown\")),\n",
    "        \"resolution\": float(dataset.x[1] - dataset.x[0]) if len(dataset.x) > 1 else 60.0,\n",
    "        \"bbox\": [\n",
    "            float(dataset.x.min()), \n",
    "            float(dataset.y.min()),\n",
    "            float(dataset.x.max()), \n",
    "            float(dataset.y.max())\n",
    "        ],\n",
    "        \"time_range\": [\n",
    "            str(dataset.time.min().values), \n",
    "            str(dataset.time.max().values)\n",
    "        ] if \"time\" in dataset.dims else None\n",
    "    }\n",
    "    \n",
    "    return {\"data\": prepared_data, \"metadata\": metadata}\n",
    "\n",
    "# Prepare the data\n",
    "prepared = prepare_for_terratorch(dataset)\n",
    "\n",
    "print(\"Data prepared for TerraTorch:\")\n",
    "print(f\"  Bands: {prepared['metadata']['bands']}\")\n",
    "print(f\"  Spatial dimensions: {prepared['metadata']['spatial_dims']}\")\n",
    "print(f\"  Temporal dimension: {prepared['metadata']['temporal_dim']}\")\n",
    "print(f\"  CRS: {prepared['metadata']['crs']}\")\n",
    "print(f\"  Resolution: {prepared['metadata']['resolution']} meters\")\n",
    "print(f\"  Bounding box: {prepared['metadata']['bbox']}\")\n",
    "print(f\"  Time range: {prepared['metadata']['time_range']}\")\n",
    "\n",
    "# Sample data shape for first band\n",
    "sample_band = list(prepared['data'].keys())[0]\n",
    "print(f\"\\nSample band '{sample_band}' shape: {prepared['data'][sample_band].shape}\")\n",
    "print(f\"Data type: {prepared['data'][sample_band].dtype}\")\n",
    "print(f\"Value range: [{prepared['data'][sample_band].min():.3f}, {prepared['data'][sample_band].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0dbaba",
   "metadata": {},
   "source": [
    "## 6. Visualize Loaded Data\n",
    "\n",
    "Let's create some visualizations to verify our data loaded correctly and to explore the temporal and spatial characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "def create_rgb_composite(dataset, time_idx=0, enhance=True):\n",
    "    \"\"\"Create an RGB composite image from the dataset.\"\"\"\n",
    "    rgb_data = np.stack([\n",
    "        dataset.red.isel(time=time_idx).values,\n",
    "        dataset.green.isel(time=time_idx).values,\n",
    "        dataset.blue.isel(time=time_idx).values\n",
    "    ], axis=-1)\n",
    "    \n",
    "    if enhance:\n",
    "        # Simple linear stretch enhancement\n",
    "        rgb_data = np.clip(rgb_data / 3000, 0, 1)  # Sentinel-2 scaling\n",
    "    \n",
    "    return rgb_data\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. RGB Composite for first time step\n",
    "ax1 = plt.subplot(2, 3, 1, projection=ccrs.PlateCarree())\n",
    "rgb_img = create_rgb_composite(dataset, time_idx=0)\n",
    "extent = [dataset.x.min(), dataset.x.max(), dataset.y.min(), dataset.y.max()]\n",
    "ax1.imshow(rgb_img, extent=extent, transform=ccrs.PlateCarree())\n",
    "ax1.add_feature(cfeature.COASTLINE, alpha=0.5)\n",
    "ax1.add_feature(cfeature.BORDERS, alpha=0.5)\n",
    "ax1.set_title(f'RGB Composite - {dataset.time[0].dt.strftime(\"%Y-%m-%d\").values}')\n",
    "ax1.gridlines(draw_labels=True, alpha=0.3)\n",
    "\n",
    "# 2. NIR band\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "nir_img = dataset.nir.isel(time=0)\n",
    "im2 = ax2.imshow(nir_img, extent=extent, cmap='Greens', vmin=0, vmax=3000)\n",
    "ax2.set_title('Near Infrared (NIR) Band')\n",
    "ax2.set_xlabel('Longitude')\n",
    "ax2.set_ylabel('Latitude')\n",
    "plt.colorbar(im2, ax=ax2, label='Reflectance')\n",
    "\n",
    "# 3. NDVI calculation and visualization\n",
    "ndvi = (dataset.nir - dataset.red) / (dataset.nir + dataset.red)\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "ndvi_img = ndvi.isel(time=0)\n",
    "im3 = ax3.imshow(ndvi_img, extent=extent, cmap='RdYlGn', vmin=-0.5, vmax=1.0)\n",
    "ax3.set_title('NDVI (Vegetation Index)')\n",
    "ax3.set_xlabel('Longitude')\n",
    "ax3.set_ylabel('Latitude')\n",
    "plt.colorbar(im3, ax=ax3, label='NDVI')\n",
    "\n",
    "# 4. Time series plot for a sample pixel\n",
    "center_x = len(dataset.x) // 2\n",
    "center_y = len(dataset.y) // 2\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "\n",
    "for band in ['red', 'green', 'blue', 'nir']:\n",
    "    time_series = dataset[band].isel(x=center_x, y=center_y)\n",
    "    ax4.plot(time_series.time, time_series.values, 'o-', label=band.upper(), alpha=0.8)\n",
    "\n",
    "ax4.set_title('Temporal Profile (Center Pixel)')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_ylabel('Reflectance')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Histogram of NDVI values\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "ndvi_flat = ndvi.isel(time=0).values.flatten()\n",
    "ndvi_clean = ndvi_flat[~np.isnan(ndvi_flat)]\n",
    "ax5.hist(ndvi_clean, bins=50, alpha=0.7, edgecolor='black')\n",
    "ax5.set_title('NDVI Distribution')\n",
    "ax5.set_xlabel('NDVI Value')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.axvline(ndvi_clean.mean(), color='red', linestyle='--', label=f'Mean: {ndvi_clean.mean():.3f}')\n",
    "ax5.legend()\n",
    "\n",
    "# 6. Cloud mask visualization (if available)\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "if hasattr(dataset, 'scl'):  # Scene Classification Layer\n",
    "    scl_img = dataset.scl.isel(time=0)\n",
    "    im6 = ax6.imshow(scl_img, extent=extent, cmap='tab10')\n",
    "    ax6.set_title('Scene Classification')\n",
    "    plt.colorbar(im6, ax=ax6, label='Class')\n",
    "else:\n",
    "    # Show data coverage\n",
    "    data_mask = ~np.isnan(dataset.red.isel(time=0).values)\n",
    "    ax6.imshow(data_mask, extent=extent, cmap='Blues')\n",
    "    ax6.set_title('Data Coverage')\n",
    "\n",
    "ax6.set_xlabel('Longitude')\n",
    "ax6.set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\\\nSummary Statistics:\")\n",
    "print(f\"Dataset temporal coverage: {len(dataset.time)} time steps\")\n",
    "print(f\"Spatial coverage: {len(dataset.x)} x {len(dataset.y)} pixels\")\n",
    "print(f\"Spatial resolution: ~{abs(dataset.x[1] - dataset.x[0]):.1f} meters\")\n",
    "\n",
    "for band in dataset.data_vars:\n",
    "    band_data = dataset[band].values\n",
    "    valid_data = band_data[~np.isnan(band_data)]\n",
    "    print(f\"{band.upper()} band: min={valid_data.min():.0f}, max={valid_data.max():.0f}, mean={valid_data.mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57000a4f",
   "metadata": {},
   "source": [
    "## 7. Create TerraTorch Dataset\n",
    "\n",
    "Now let's demonstrate how to create a TerraTorch-compatible dataset from our loaded data. This prepares the data for use with geospatial foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock TerraTorch dataset class for demonstration\n",
    "class ODCSTACDataset:\n",
    "    \"\"\"\n",
    "    A demonstration dataset class that wraps odc-stac loaded data\n",
    "    for use with TerraTorch workflows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset: xr.Dataset, tile_size: int = 256):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset: xarray Dataset from odc-stac\n",
    "            tile_size: Size of tiles to extract for training\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.tile_size = tile_size\n",
    "        self.bands = list(dataset.data_vars.keys())\n",
    "        \n",
    "        # Calculate number of possible tiles\n",
    "        self.tiles_x = (len(dataset.x) - tile_size) // tile_size + 1\n",
    "        self.tiles_y = (len(dataset.y) - tile_size) // tile_size + 1\n",
    "        self.n_times = len(dataset.time) if 'time' in dataset.dims else 1\n",
    "        \n",
    "        print(f\"Dataset initialized:\")\n",
    "        print(f\"  Total tiles per timestep: {self.tiles_x * self.tiles_y}\")\n",
    "        print(f\"  Time steps: {self.n_times}\")\n",
    "        print(f\"  Total samples: {self.tiles_x * self.tiles_y * self.n_times}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of samples.\"\"\"\n",
    "        return self.tiles_x * self.tiles_y * self.n_times\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample (tile) from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Sample index\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'image' and metadata\n",
    "        \"\"\"\n",
    "        # Calculate tile position and time index\n",
    "        samples_per_time = self.tiles_x * self.tiles_y\n",
    "        time_idx = idx // samples_per_time\n",
    "        tile_idx = idx % samples_per_time\n",
    "        \n",
    "        tile_y = tile_idx // self.tiles_x\n",
    "        tile_x = tile_idx % self.tiles_x\n",
    "        \n",
    "        # Extract tile coordinates\n",
    "        x_start = tile_x * self.tile_size\n",
    "        x_end = x_start + self.tile_size\n",
    "        y_start = tile_y * self.tile_size\n",
    "        y_end = y_start + self.tile_size\n",
    "        \n",
    "        # Extract data for this tile\n",
    "        tile_data = {}\n",
    "        for band in self.bands:\n",
    "            if 'time' in self.dataset.dims:\n",
    "                band_data = self.dataset[band].isel(time=time_idx, x=slice(x_start, x_end), y=slice(y_start, y_end))\n",
    "            else:\n",
    "                band_data = self.dataset[band].isel(x=slice(x_start, x_end), y=slice(y_start, y_end))\n",
    "            \n",
    "            # Convert to numpy and normalize\n",
    "            band_array = band_data.values.astype(np.float32) / 10000.0  # Normalize Sentinel-2\n",
    "            tile_data[band] = np.clip(band_array, 0, 1)\n",
    "        \n",
    "        # Stack bands into a single array (C, H, W format for PyTorch)\n",
    "        image = np.stack([tile_data[band] for band in self.bands], axis=0)\n",
    "        \n",
    "        # Get coordinates\n",
    "        x_coords = self.dataset.x.isel(x=slice(x_start, x_end)).values\n",
    "        y_coords = self.dataset.y.isel(y=slice(y_start, y_end)).values\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'bands': self.bands,\n",
    "            'coordinates': {\n",
    "                'x': x_coords,\n",
    "                'y': y_coords,\n",
    "                'time_idx': time_idx\n",
    "            },\n",
    "            'metadata': {\n",
    "                'tile_x': tile_x,\n",
    "                'tile_y': tile_y,\n",
    "                'shape': image.shape\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_sample_batch(self, batch_size: int = 4):\n",
    "        \"\"\"Get a batch of samples for demonstration.\"\"\"\n",
    "        indices = np.random.choice(len(self), size=batch_size, replace=False)\n",
    "        batch = [self[idx] for idx in indices]\n",
    "        return batch\n",
    "\n",
    "# Create the dataset\n",
    "odc_dataset = ODCSTACDataset(dataset, tile_size=128)\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch = odc_dataset.get_sample_batch(batch_size=4)\n",
    "\n",
    "print(f\"\\nSample batch information:\")\n",
    "for i, sample in enumerate(sample_batch):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Image shape: {sample['image'].shape}\")\n",
    "    print(f\"  Bands: {sample['bands']}\")\n",
    "    print(f\"  Tile position: ({sample['metadata']['tile_x']}, {sample['metadata']['tile_y']})\")\n",
    "    print(f\"  Value range: [{sample['image'].min():.3f}, {sample['image'].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a659bb2",
   "metadata": {},
   "source": [
    "## 8. Demonstrate TerraTorch Functionality\n",
    "\n",
    "Finally, let's demonstrate some basic TerraTorch-style functionality with our loaded data, including data iteration, batching, and preprocessing transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data iteration and preprocessing\n",
    "def apply_transforms(sample, augment=True):\n",
    "    \"\"\"\n",
    "    Apply preprocessing transforms similar to TerraTorch workflows.\n",
    "    \n",
    "    Args:\n",
    "        sample: Sample dictionary from dataset\n",
    "        augment: Whether to apply data augmentation\n",
    "        \n",
    "    Returns:\n",
    "        Transformed sample\n",
    "    \"\"\"\n",
    "    image = sample['image'].copy()\n",
    "    \n",
    "    if augment:\n",
    "        # Random horizontal flip\n",
    "        if np.random.rand() > 0.5:\n",
    "            image = np.flip(image, axis=2)  # Flip along width\n",
    "        \n",
    "        # Random vertical flip\n",
    "        if np.random.rand() > 0.5:\n",
    "            image = np.flip(image, axis=1)  # Flip along height\n",
    "        \n",
    "        # Small rotation (simplified)\n",
    "        if np.random.rand() > 0.7:\n",
    "            # 90-degree rotation\n",
    "            image = np.rot90(image, axes=(1, 2))\n",
    "    \n",
    "    # Normalize to standard range\n",
    "    image = (image - 0.1) / 0.3  # Example normalization\n",
    "    image = np.clip(image, -2, 2)\n",
    "    \n",
    "    sample_transformed = sample.copy()\n",
    "    sample_transformed['image'] = image\n",
    "    sample_transformed['transforms_applied'] = True\n",
    "    \n",
    "    return sample_transformed\n",
    "\n",
    "# Demonstrate batching and preprocessing\n",
    "print(\"Demonstrating data iteration and preprocessing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create multiple batches\n",
    "for batch_idx in range(3):\n",
    "    print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = odc_dataset.get_sample_batch(batch_size=2)\n",
    "    \n",
    "    # Apply transforms\n",
    "    transformed_batch = [apply_transforms(sample, augment=True) for sample in batch]\n",
    "    \n",
    "    # Display batch statistics\n",
    "    for i, sample in enumerate(transformed_batch):\n",
    "        image = sample['image']\n",
    "        print(f\"  Sample {i+1}: shape={image.shape}, range=[{image.min():.2f}, {image.max():.2f}]\")\n",
    "        print(f\"    Tile position: ({sample['metadata']['tile_x']}, {sample['metadata']['tile_y']})\")\n",
    "\n",
    "# Visualize some transformed samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "sample_batch = odc_dataset.get_sample_batch(batch_size=4)\n",
    "transformed_batch = [apply_transforms(sample, augment=False) for sample in sample_batch]\n",
    "\n",
    "for i, (original, transformed) in enumerate(zip(sample_batch, transformed_batch)):\n",
    "    # Original RGB\n",
    "    rgb_original = np.transpose(original['image'][:3], (1, 2, 0))  # Convert to HWC\n",
    "    rgb_original = np.clip(rgb_original, 0, 1)\n",
    "    axes[0, i].imshow(rgb_original)\n",
    "    axes[0, i].set_title(f'Original {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Transformed RGB (denormalize for display)\n",
    "    rgb_transformed = np.transpose(transformed['image'][:3], (1, 2, 0))\n",
    "    rgb_transformed = (rgb_transformed * 0.3) + 0.1  # Reverse normalization\n",
    "    rgb_transformed = np.clip(rgb_transformed, 0, 1)\n",
    "    axes[1, i].imshow(rgb_transformed)\n",
    "    axes[1, i].set_title(f'Transformed {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Original vs. Transformed Samples', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Summary of the complete workflow\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì Connected to STAC catalog (Planetary Computer)\")\n",
    "print(\"‚úì Searched for Sentinel-2 imagery with cloud filtering\")\n",
    "print(\"‚úì Loaded multi-temporal data using odc-stac into xarray\")\n",
    "print(\"‚úì Converted to TerraTorch-compatible format\")\n",
    "print(\"‚úì Created visualizations and exploratory analysis\")\n",
    "print(\"‚úì Built custom dataset class for ML workflows\")\n",
    "print(\"‚úì Demonstrated data iteration and preprocessing\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"- Integrate with actual TerraTorch models\")\n",
    "print(\"- Implement custom loss functions for geospatial tasks\")\n",
    "print(\"- Scale to larger datasets and cloud processing\")\n",
    "print(\"- Add multi-modal data sources (SAR, hyperspectral)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a3428",
   "metadata": {},
   "source": [
    "## 9. Generate TerraMind Embeddings\n",
    "\n",
    "Now let's demonstrate generating embeddings using TerraMind, IBM's geospatial foundation model, from our loaded satellite imagery. TerraMind can generate rich 768-dimensional embeddings from 16x16 pixel patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8729bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# TerraMind preprocessing functions\n",
    "def rgb_smooth_quantiles(rgb_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply smooth quantile normalization to RGB array.\n",
    "    \n",
    "    Args:\n",
    "        rgb_array: RGB array of shape (..., 3)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized RGB array\n",
    "    \"\"\"\n",
    "    # Calculate smooth quantiles (2% and 98%)\n",
    "    lower_quantile = np.percentile(rgb_array, 2, axis=(0, 1), keepdims=True)\n",
    "    upper_quantile = np.percentile(rgb_array, 98, axis=(0, 1), keepdims=True)\n",
    "    \n",
    "    # Clip and normalize to [0, 1]\n",
    "    clipped = np.clip(rgb_array, lower_quantile, upper_quantile)\n",
    "    normalized = (clipped - lower_quantile) / (upper_quantile - lower_quantile + 1e-8)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def prepare_terramind_patches(rgb_data: np.ndarray, patch_size: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare RGB data for TerraMind inference by extracting patches.\n",
    "    \n",
    "    Args:\n",
    "        rgb_data: RGB data array of shape (H, W, 3)\n",
    "        patch_size: Size of square patches (default 16x16)\n",
    "    \n",
    "    Returns:\n",
    "        Array of patches of shape (num_patches, patch_size, patch_size, 3)\n",
    "    \"\"\"\n",
    "    height, width, channels = rgb_data.shape\n",
    "    \n",
    "    # Calculate number of patches\n",
    "    num_patches_h = height // patch_size\n",
    "    num_patches_w = width // patch_size\n",
    "    \n",
    "    # Crop to fit exact patches\n",
    "    cropped_h = num_patches_h * patch_size\n",
    "    cropped_w = num_patches_w * patch_size\n",
    "    cropped_data = rgb_data[:cropped_h, :cropped_w, :]\n",
    "    \n",
    "    # Reshape into patches\n",
    "    patches = cropped_data.reshape(\n",
    "        num_patches_h, patch_size, num_patches_w, patch_size, channels\n",
    "    ).transpose(0, 2, 1, 3, 4).reshape(-1, patch_size, patch_size, channels)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def normalize_terramind_input(patches: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize patches for TerraMind input.\n",
    "    TerraMind expects inputs normalized with ImageNet statistics.\n",
    "    \n",
    "    Args:\n",
    "        patches: Array of patches of shape (num_patches, 16, 16, 3)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized tensor of shape (num_patches, 3, 16, 16)\n",
    "    \"\"\"\n",
    "    # ImageNet normalization parameters\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    patches_tensor = torch.from_numpy(patches).float()\n",
    "    \n",
    "    # Transpose to (batch, channels, height, width)\n",
    "    patches_tensor = patches_tensor.permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Normalize\n",
    "    for i in range(3):\n",
    "        patches_tensor[:, i] = (patches_tensor[:, i] - mean[i]) / std[i]\n",
    "    \n",
    "    return patches_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da17127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TerraMind model\n",
    "try:\n",
    "    from terratorch.models.backbones import BACKBONE_REGISTRY\n",
    "    \n",
    "    # Initialize TerraMind model\n",
    "    print(\"Loading TerraMind model...\")\n",
    "    model = BACKBONE_REGISTRY.build(\n",
    "        \"terramind_v1_base\", \n",
    "        modalities=[\"S2RGB\"], \n",
    "        pretrained=True\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully: {type(model)}\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        model = model.to(device)\n",
    "        print(\"Model moved to GPU\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Could not load TerraMind model: {e}\")\n",
    "    print(\"Please ensure terratorch is installed: pip install terratorch\")\n",
    "    model = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading TerraMind model: {e}\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67932274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_terramind_embeddings(rgb_data: np.ndarray, model, patch_size: int = 16, batch_size: int = 32) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate TerraMind embeddings for RGB satellite imagery.\n",
    "    \n",
    "    Args:\n",
    "        rgb_data: RGB data array of shape (H, W, 3), values in [0, 1]\n",
    "        model: Loaded TerraMind model\n",
    "        patch_size: Size of patches for processing\n",
    "        batch_size: Batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (embeddings array, metadata dict)\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        raise ValueError(\"Model not loaded. Cannot generate embeddings.\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Step 1: Apply smooth quantile normalization\n",
    "    print(\"Applying smooth quantile normalization...\")\n",
    "    normalized_rgb = rgb_smooth_quantiles(rgb_data)\n",
    "    \n",
    "    # Step 2: Extract patches\n",
    "    print(f\"Extracting {patch_size}x{patch_size} patches...\")\n",
    "    patches = prepare_terramind_patches(normalized_rgb, patch_size)\n",
    "    print(f\"Extracted {len(patches)} patches\")\n",
    "    \n",
    "    # Step 3: Normalize for model input\n",
    "    print(\"Normalizing patches for model input...\")\n",
    "    patches_tensor = normalize_terramind_input(patches)\n",
    "    \n",
    "    # Step 4: Generate embeddings in batches\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(patches_tensor), batch_size):\n",
    "            batch = patches_tensor[i:i+batch_size].to(device)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            batch_embeddings = model({\"S2RGB\": batch})\n",
    "            \n",
    "            # Move back to CPU and store\n",
    "            embeddings_list.append(batch_embeddings.cpu().numpy())\n",
    "            \n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + len(batch)}/{len(patches_tensor)} patches\")\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"num_patches\": len(patches),\n",
    "        \"patch_size\": patch_size,\n",
    "        \"embedding_dim\": embeddings.shape[1],\n",
    "        \"original_shape\": rgb_data.shape,\n",
    "        \"patches_shape\": patches.shape,\n",
    "        \"device_used\": str(device)\n",
    "    }\n",
    "    \n",
    "    print(f\"Generated {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")\n",
    "    return embeddings, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de257f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings from our Auckland data\n",
    "if model is not None and len(datasets) > 0:\n",
    "    # Get the most recent image with good quality\n",
    "    latest_dataset = datasets[-1]  # Most recent\n",
    "    \n",
    "    # Create RGB composite\n",
    "    rgb_array = create_rgb_composite(latest_dataset).values\n",
    "    \n",
    "    # Ensure we have valid data\n",
    "    if not np.isnan(rgb_array).all():\n",
    "        print(f\"Generating TerraMind embeddings for Auckland data...\")\n",
    "        print(f\"RGB data shape: {rgb_array.shape}\")\n",
    "        print(f\"RGB data range: [{np.nanmin(rgb_array):.3f}, {np.nanmax(rgb_array):.3f}]\")\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings\n",
    "            embeddings, metadata = generate_terramind_embeddings(\n",
    "                rgb_array, \n",
    "                model, \n",
    "                patch_size=16, \n",
    "                batch_size=16  # Smaller batch size for safety\n",
    "            )\n",
    "            \n",
    "            print(\"\\nEmbedding Generation Results:\")\n",
    "            print(f\"Generated {len(embeddings)} embeddings\")\n",
    "            print(f\"Embedding shape: {embeddings.shape}\")\n",
    "            print(f\"Embedding statistics:\")\n",
    "            print(f\"  Mean: {np.mean(embeddings):.4f}\")\n",
    "            print(f\"  Std: {np.std(embeddings):.4f}\")\n",
    "            print(f\"  Min: {np.min(embeddings):.4f}\")\n",
    "            print(f\"  Max: {np.max(embeddings):.4f}\")\n",
    "            \n",
    "            # Print metadata\n",
    "            print(f\"\\nMetadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "                \n",
    "            # Store embeddings for later use\n",
    "            auckland_embeddings = embeddings\n",
    "            auckland_metadata = metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            auckland_embeddings = None\n",
    "            auckland_metadata = None\n",
    "    else:\n",
    "        print(\"No valid data found for embedding generation\")\n",
    "        auckland_embeddings = None\n",
    "        auckland_metadata = None\n",
    "else:\n",
    "    print(\"Skipping embedding generation (no model or no data)\")\n",
    "    auckland_embeddings = None\n",
    "    auckland_metadata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ea98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and analyze the embeddings\n",
    "if auckland_embeddings is not None:\n",
    "    # Create visualization of embedding statistics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Embedding distribution\n",
    "    axes[0, 0].hist(auckland_embeddings.flatten(), bins=50, alpha=0.7)\n",
    "    axes[0, 0].set_title('Distribution of Embedding Values')\n",
    "    axes[0, 0].set_xlabel('Embedding Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 2. Mean embedding per patch\n",
    "    patch_means = np.mean(auckland_embeddings, axis=1)\n",
    "    axes[0, 1].hist(patch_means, bins=30, alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_title('Mean Embedding Value per Patch')\n",
    "    axes[0, 1].set_xlabel('Mean Embedding Value')\n",
    "    axes[0, 1].set_ylabel('Number of Patches')\n",
    "    \n",
    "    # 3. Embedding dimension variance\n",
    "    dim_variance = np.var(auckland_embeddings, axis=0)\n",
    "    axes[1, 0].plot(dim_variance)\n",
    "    axes[1, 0].set_title('Variance Across Embedding Dimensions')\n",
    "    axes[1, 0].set_xlabel('Embedding Dimension')\n",
    "    axes[1, 0].set_ylabel('Variance')\n",
    "    \n",
    "    # 4. Sample embedding heatmap\n",
    "    sample_embeddings = auckland_embeddings[:min(50, len(auckland_embeddings))]\n",
    "    im = axes[1, 1].imshow(sample_embeddings.T, aspect='auto', cmap='viridis')\n",
    "    axes[1, 1].set_title('Sample Embeddings Heatmap')\n",
    "    axes[1, 1].set_xlabel('Patch Index')\n",
    "    axes[1, 1].set_ylabel('Embedding Dimension')\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('TerraMind Embedding Analysis for Auckland Satellite Data', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate some interesting statistics\n",
    "    print(f\"\\nEmbedding Analysis:\")\n",
    "    print(f\"Total patches processed: {len(auckland_embeddings)}\")\n",
    "    print(f\"Embedding dimensionality: {auckland_embeddings.shape[1]}\")\n",
    "    print(f\"Most variable dimensions: {np.argsort(dim_variance)[-5:]}\")\n",
    "    print(f\"Least variable dimensions: {np.argsort(dim_variance)[:5]}\")\n",
    "    \n",
    "    # Calculate patch similarity (cosine similarity between first few patches)\n",
    "    if len(auckland_embeddings) > 1:\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        # Calculate similarity for first 10 patches\n",
    "        n_samples = min(10, len(auckland_embeddings))\n",
    "        similarity_matrix = cosine_similarity(auckland_embeddings[:n_samples])\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.colorbar(label='Cosine Similarity')\n",
    "        plt.title(f'Cosine Similarity Between First {n_samples} Patches')\n",
    "        plt.xlabel('Patch Index')\n",
    "        plt.ylabel('Patch Index')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Average cosine similarity between patches: {np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]):.3f}\")\n",
    "else:\n",
    "    print(\"No embeddings available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6c915",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully completed the full pipeline from STAC data loading to TerraMind embedding generation. Here's what we accomplished:\n",
    "\n",
    "### What We Built:\n",
    "1. **STAC Data Loading**: Connected to Microsoft Planetary Computer and loaded Sentinel-2 data for Auckland\n",
    "2. **Data Preprocessing**: Applied cloud filtering, temporal selection, and band extraction\n",
    "3. **Visualization**: Created RGB composites, NDVI analysis, and time series plots\n",
    "4. **TerraTorch Integration**: Prepared data for machine learning workflows\n",
    "5. **TerraMind Embeddings**: Generated 768-dimensional embeddings from satellite imagery patches\n",
    "\n",
    "### Key Achievements:\n",
    "- üåç **Global Scale**: Works with any geographic region through STAC catalogs\n",
    "- ‚òÅÔ∏è **Cloud-Native**: Leverages cloud-optimized data formats and catalogs\n",
    "- ü§ñ **Foundation Models**: Integrates state-of-the-art geospatial AI models\n",
    "- üìä **Rich Analytics**: Provides comprehensive data analysis and visualization\n",
    "- üîÑ **Reproducible**: Fully documented workflow with configuration management\n",
    "\n",
    "### Potential Applications:\n",
    "- **Change Detection**: Compare embeddings across time to detect changes\n",
    "- **Land Cover Classification**: Use embeddings as features for ML models\n",
    "- **Similarity Search**: Find similar geographic regions using embedding similarity\n",
    "- **Anomaly Detection**: Identify unusual patterns in satellite imagery\n",
    "- **Multi-temporal Analysis**: Track environmental changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcdba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for future use\n",
    "if auckland_embeddings is not None:\n",
    "    import pickle\n",
    "    \n",
    "    # Save complete results\n",
    "    results = {\n",
    "        \"embeddings\": auckland_embeddings,\n",
    "        \"metadata\": auckland_metadata,\n",
    "        \"rgb_composite\": rgb_array,\n",
    "        \"dataset_info\": {\n",
    "            \"dims\": dict(datasets[-1].dims),\n",
    "            \"coords\": list(datasets[-1].coords.keys()),\n",
    "            \"data_vars\": list(datasets[-1].data_vars.keys())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = \"outputs/auckland_terramind_results.pkl\"\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"Complete results saved to: {output_file}\")\n",
    "    print(\"You can load these results later with:\")\n",
    "    print(f\"with open('{output_file}', 'rb') as f:\")\n",
    "    print(\"    results = pickle.load(f)\")\n",
    "else:\n",
    "    print(\"No results to save (embeddings generation failed)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
